{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aJuUrH69y6D1"
   },
   "source": [
    "<h1><font color=darkcyan>Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "2hWn-Roayvk3"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install transformers\n",
    "\n",
    "# PyTorch libraries\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Tensorflow libraries\n",
    "import tensorflow as tf\n",
    "tf.config.run_functions_eagerly(True)\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from time import time\n",
    "import seaborn as sns\n",
    "\n",
    "import IPython\n",
    "from IPython.display import clear_output \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Il2QLdhMzil_"
   },
   "source": [
    "# <h1><font color=indigo>Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "gBSdb6rbzlmh"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('/content/drive/MyDrive/Emotion_final.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "id": "a1_wUU71SOzw",
    "outputId": "889aff96-7239-4753-bb75-3bfc987d9aa8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Emotion\n",
       "0                            i didnt feel humiliated  sadness\n",
       "1  i can go from feeling so hopeless to so damned...  sadness\n",
       "2   im grabbing a minute to post i feel greedy wrong    anger\n",
       "3  i am ever feeling nostalgic about the fireplac...     love\n",
       "4                               i am feeling grouchy    anger"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UQUm48fYU1ZA"
   },
   "source": [
    "# <h1><font color=darkmagneta>Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 349
    },
    "id": "Mz1bjnvOUEEQ",
    "outputId": "108e02f8-871b-46d2-cea8-0f7832b42779"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f2b1d26be50>"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(data['Emotion'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EYewVjG7kXDT"
   },
   "source": [
    "# <h3><font color>Histogram of sequence lenght\n",
    "To find the Appropriate max lenght for training phase, we use the histogram below.\n",
    "The horizontal axis shows the lenght of a sentence and the vertical axis shows frequency of samples with that lenght. \n",
    "As we can see, the proper maximum lenght to be passed to our neural network model is about 40.\n",
    "We can choose bigger lenght but it doesnt have much impact on the accuracy( it only increases ram usage )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 329
    },
    "id": "NSqCVMfUg6GG",
    "outputId": "939df393-4830-412b-8ddb-0d5d2779a30f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0, 980.0)"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get length of all the messages in the train set\n",
    "seq_len = [len(i.split()) for i in data['Text']]\n",
    "\n",
    "n, bins, patches = plt.hist(seq_len, bins='auto', color='steelblue',\n",
    "                            alpha=1, rwidth=0.85)\n",
    "print(len(bins))\n",
    "plt.grid(axis='y', alpha=0.2)\n",
    "plt.xlabel('sequence lenght')\n",
    "plt.ylabel('counts')\n",
    "plt.title('Histogram')\n",
    "\n",
    "maxfreq = n.max()\n",
    "# Set a clean upper y-axis limit.\n",
    "plt.ylim(ymax=np.ceil(maxfreq / 9) * 10 if maxfreq % 10 else maxfreq + 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "NRr3Ha2oCBwT",
    "outputId": "ff3f6084-7dcd-4735-adc0-1561effd0046"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# example of kernel density estimation for a bimodal data sample\n",
    "from matplotlib import pyplot\n",
    "from numpy.random import normal\n",
    "from numpy import hstack\n",
    "from numpy import asarray\n",
    "from numpy import exp\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "sample = np.array(seq_len).reshape(-1,1)\n",
    "# fit density\n",
    "model = KernelDensity(bandwidth=3, kernel='gaussian')\n",
    "sample = sample.reshape((len(sample), 1))\n",
    "model.fit(sample)\n",
    "# sample probabilities for a range of outcomes\n",
    "values = asarray([value for value in range(-1, 65)])\n",
    "values = values.reshape((len(values), 1))\n",
    "probabilities = model.score_samples(values)\n",
    "probabilities = exp(probabilities)\n",
    "# plot the histogram and pdf\n",
    "pyplot.hist(sample, bins=65, density=True , color = 'aliceblue')\n",
    "pyplot.plot(values[:], probabilities)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sPNxYyXc1c4a"
   },
   "source": [
    "## removing punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "BmonFz-2OOtX",
    "outputId": "12df98c3-c99f-4e64-b769-0cc26ccb724f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "id": "3EhdODQDORUt",
    "outputId": "60e83931-3fdf-4603-a665-1a6953a48c13"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>text_wo_punct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>sadness</td>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>anger</td>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>love</td>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>anger</td>\n",
       "      <td>i am feeling grouchy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  ...                                      text_wo_punct\n",
       "0                            i didnt feel humiliated  ...                            i didnt feel humiliated\n",
       "1  i can go from feeling so hopeless to so damned...  ...  i can go from feeling so hopeless to so damned...\n",
       "2   im grabbing a minute to post i feel greedy wrong  ...   im grabbing a minute to post i feel greedy wrong\n",
       "3  i am ever feeling nostalgic about the fireplac...  ...  i am ever feeling nostalgic about the fireplac...\n",
       "4                               i am feeling grouchy  ...                               i am feeling grouchy\n",
       "\n",
       "[5 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_punctuation(text):\n",
    "    no_punct=[words for words in text if words not in string.punctuation]\n",
    "    words_wo_punct=''.join(no_punct)\n",
    "    return words_wo_punct\n",
    "data['text_wo_punct']=data['Text'].apply(lambda x: remove_punctuation(x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TqnSDVzb1ZwC"
   },
   "source": [
    "## tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 252
    },
    "id": "_eE8fgLfzMSh",
    "outputId": "6578f494-e191-46d9-bbe1-70aae187da33"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>text_wo_punct</th>\n",
       "      <th>text_wo_punct_split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>sadness</td>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>[i, didnt, feel, humiliated]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>[i, can, go, from, feeling, so, hopeless, to, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>anger</td>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>[im, grabbing, a, minute, to, post, i, feel, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>love</td>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>[i, am, ever, feeling, nostalgic, about, the, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>anger</td>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>[i, am, feeling, grouchy]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  ...                                text_wo_punct_split\n",
       "0                            i didnt feel humiliated  ...                       [i, didnt, feel, humiliated]\n",
       "1  i can go from feeling so hopeless to so damned...  ...  [i, can, go, from, feeling, so, hopeless, to, ...\n",
       "2   im grabbing a minute to post i feel greedy wrong  ...  [im, grabbing, a, minute, to, post, i, feel, g...\n",
       "3  i am ever feeling nostalgic about the fireplac...  ...  [i, am, ever, feeling, nostalgic, about, the, ...\n",
       "4                               i am feeling grouchy  ...                          [i, am, feeling, grouchy]\n",
       "\n",
       "[5 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(text):\n",
    "    split=re.split(\"\\W+\",text) \n",
    "    return split\n",
    "data['text_wo_punct_split']=data['text_wo_punct'].apply(lambda x: tokenize(x.lower()))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "F_3AAgCC1WzV"
   },
   "outputs": [],
   "source": [
    "## removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 302
    },
    "id": "RKE3bCc0X0mZ",
    "outputId": "7e89bc76-3ebf-4472-e6ff-476b1112d749"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\"]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>text_wo_punct</th>\n",
       "      <th>text_wo_punct_split</th>\n",
       "      <th>text_wo_punct_split_wo_stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>sadness</td>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>[i, didnt, feel, humiliated]</td>\n",
       "      <td>[didnt, feel, humiliated]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>[i, can, go, from, feeling, so, hopeless, to, ...</td>\n",
       "      <td>[go, feeling, hopeless, damned, hopeful, aroun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>anger</td>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>[im, grabbing, a, minute, to, post, i, feel, g...</td>\n",
       "      <td>[im, grabbing, minute, post, feel, greedy, wrong]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>love</td>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>[i, am, ever, feeling, nostalgic, about, the, ...</td>\n",
       "      <td>[ever, feeling, nostalgic, fireplace, know, st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>anger</td>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>[i, am, feeling, grouchy]</td>\n",
       "      <td>[feeling, grouchy]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  ...                   text_wo_punct_split_wo_stopwords\n",
       "0                            i didnt feel humiliated  ...                          [didnt, feel, humiliated]\n",
       "1  i can go from feeling so hopeless to so damned...  ...  [go, feeling, hopeless, damned, hopeful, aroun...\n",
       "2   im grabbing a minute to post i feel greedy wrong  ...  [im, grabbing, minute, post, feel, greedy, wrong]\n",
       "3  i am ever feeling nostalgic about the fireplac...  ...  [ever, feeling, nostalgic, fireplace, know, st...\n",
       "4                               i am feeling grouchy  ...                                 [feeling, grouchy]\n",
       "\n",
       "[5 rows x 5 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stopword = nltk.corpus.stopwords.words('english')\n",
    "print(stopword[:11])\n",
    "def remove_stopwords(text):\n",
    "    text=[word for word in text if word not in stopword]\n",
    "    return text\n",
    "data['text_wo_punct_split_wo_stopwords'] = data['text_wo_punct_split'].apply(lambda x: remove_stopwords(x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LGUmzoxl1Qya"
   },
   "source": [
    "## lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G7si4Oln1aWn",
    "outputId": "00f731d6-be97-4ed0-d9da-99990ea56dea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# import these modules\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(w) for w in text]  ##Notice the use of text.\n",
    "\n",
    "def join_lem(text):\n",
    "  return ' '.join(w for w in text)\n",
    "\n",
    "data['Text'] = data[\"text_wo_punct_split_wo_stopwords\"].apply(lemmatize_text)\n",
    "data['Text'] = data['Text'].apply(join_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "yIrF935n0Nfd"
   },
   "outputs": [],
   "source": [
    "data = data.filter(['Text','Emotion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 415
    },
    "id": "YHXxIOBx1sso",
    "outputId": "84d5f629-7c48-4d02-c9b8-e93cbdb769db"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>didnt feel humiliated</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>go feeling hopeless damned hopeful around some...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing minute post feel greedy wrong</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ever feeling nostalgic fireplace know still pr...</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>feeling grouchy</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21454</th>\n",
       "      <td>melissa stared friend dism</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21455</th>\n",
       "      <td>successive state election seen governing party...</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21456</th>\n",
       "      <td>vincent irritated dismay</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21457</th>\n",
       "      <td>kendallhume turned back face dismayed coup</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21458</th>\n",
       "      <td>dismayed surpris</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21459 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text  Emotion\n",
       "0                                  didnt feel humiliated  sadness\n",
       "1      go feeling hopeless damned hopeful around some...  sadness\n",
       "2              im grabbing minute post feel greedy wrong    anger\n",
       "3      ever feeling nostalgic fireplace know still pr...     love\n",
       "4                                        feeling grouchy    anger\n",
       "...                                                  ...      ...\n",
       "21454                         melissa stared friend dism     fear\n",
       "21455  successive state election seen governing party...     fear\n",
       "21456                           vincent irritated dismay     fear\n",
       "21457         kendallhume turned back face dismayed coup     fear\n",
       "21458                                   dismayed surpris     fear\n",
       "\n",
       "[21459 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YGSwtSOXz_bc"
   },
   "source": [
    "Since the machine learning model can only process numerical data, we need to convert the emotion column strings into a numerical format.\n",
    "We then add a new column with corresponding numerical value to each emotion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YJEW4z5jz_bb",
    "outputId": "b9984ee2-b9f7-457c-cc0a-2f950169d200"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'anger': 1, 'fear': 4, 'happy': 5, 'love': 2, 'sadness': 0, 'surprise': 3}"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "possible_labels = data.Emotion.unique()\n",
    "\n",
    "label_dict = {}\n",
    "for index, possible_label in enumerate(possible_labels):\n",
    "    label_dict[possible_label] = index\n",
    "label_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 415
    },
    "id": "csv2SEs5z_bc",
    "outputId": "320ac71c-88ca-46cd-c35a-59d52033c66d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>didnt feel humiliated</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>go feeling hopeless damned hopeful around some...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing minute post feel greedy wrong</td>\n",
       "      <td>anger</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ever feeling nostalgic fireplace know still pr...</td>\n",
       "      <td>love</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>feeling grouchy</td>\n",
       "      <td>anger</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21454</th>\n",
       "      <td>melissa stared friend dism</td>\n",
       "      <td>fear</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21455</th>\n",
       "      <td>successive state election seen governing party...</td>\n",
       "      <td>fear</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21456</th>\n",
       "      <td>vincent irritated dismay</td>\n",
       "      <td>fear</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21457</th>\n",
       "      <td>kendallhume turned back face dismayed coup</td>\n",
       "      <td>fear</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21458</th>\n",
       "      <td>dismayed surpris</td>\n",
       "      <td>fear</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21459 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text  Emotion  label\n",
       "0                                  didnt feel humiliated  sadness      0\n",
       "1      go feeling hopeless damned hopeful around some...  sadness      0\n",
       "2              im grabbing minute post feel greedy wrong    anger      1\n",
       "3      ever feeling nostalgic fireplace know still pr...     love      2\n",
       "4                                        feeling grouchy    anger      1\n",
       "...                                                  ...      ...    ...\n",
       "21454                         melissa stared friend dism     fear      4\n",
       "21455  successive state election seen governing party...     fear      4\n",
       "21456                           vincent irritated dismay     fear      4\n",
       "21457         kendallhume turned back face dismayed coup     fear      4\n",
       "21458                                   dismayed surpris     fear      4\n",
       "\n",
       "[21459 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['label'] = data.Emotion.replace(label_dict)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 329
    },
    "id": "BvzO3E7861Lv",
    "outputId": "2b085cc2-9fdc-4d01-9082-d2679bf5b2e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0, 2110.0)"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get length of all the messages in the train set\n",
    "seq_len = [len(i.split()) for i in data['Text']]\n",
    "\n",
    "n, bins, patches = plt.hist(seq_len, bins='auto', color='steelblue',\n",
    "                            alpha=1, rwidth=0.85)\n",
    "print(len(bins))\n",
    "plt.grid(axis='y', alpha=0.2)\n",
    "plt.xlabel('sequence lenght')\n",
    "plt.ylabel('counts')\n",
    "plt.title('Histogram')\n",
    "\n",
    "maxfreq = n.max()\n",
    "# Set a clean upper y-axis limit.\n",
    "plt.ylim(ymax=np.ceil(maxfreq / 9) * 10 if maxfreq % 10 else maxfreq + 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "SIijuFFDkBj_"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, valid_test = train_test_split(data, test_size=0.4 , shuffle=True)\n",
    "\n",
    "validate , test = train_test_split(valid_test, test_size=0.5 , shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9s5sHzVh4J3L"
   },
   "source": [
    "## <h1><font color=indigo>1.4 Converting to Prefetched dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "v_DouzCeafzR"
   },
   "outputs": [],
   "source": [
    "def convert_to_TF(data):\n",
    "  data = tf.data.Dataset.from_tensor_slices((data['Text'],data['label']))\n",
    "  data = data.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "  return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "A0XJ31Oqttgf"
   },
   "outputs": [],
   "source": [
    "# CONVERT TO TensorFlow DATASETS\n",
    "train_dataset = convert_to_TF(train)\n",
    "\n",
    "validation_dataset = convert_to_TF(validate)\n",
    "\n",
    "test_dataset = convert_to_TF(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EuMeDJrOBh7p"
   },
   "source": [
    "# **1. Matching Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Vdlj0LlCE_C"
   },
   "source": [
    "## <h1><font color='darkcyan'>1.1 Pre proccessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fJ6MvP66DFBV"
   },
   "source": [
    "## <h1><font color='darkcyan'>1.2 Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "zwq0-RHh-2xo"
   },
   "outputs": [],
   "source": [
    "def map_to_dict(input_ids, attention_mask, label):\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask\n",
    "    }, label\n",
    "\n",
    "\n",
    "def encode_examples(ds, max_sequence_length):\n",
    "    # prepare list, so that we can build up final TensorFlow dataset from slices.\n",
    "    input_ids_list = []\n",
    "    attention_mask_list = []\n",
    "    labels = []\n",
    "\n",
    "    #iterate over ds and extract input ids and attention masks for each sample\n",
    "    for review, label in tfds.as_numpy(ds):\n",
    "        bert_input = tokenizer( \n",
    "                        review.decode(),                    \n",
    "                        add_special_tokens = True, # add [CLS], [SEP] default is True\n",
    "                        max_length = max_sequence_length, # max length of the text that can go to BERT\n",
    "                        padding='max_length', # add [PAD] tokens\n",
    "                        return_attention_mask = True, # add attention mask to not focus on pad tokens\n",
    "                        truncation = True\n",
    "                       )\n",
    "        input_ids_list.append(bert_input['input_ids'])\n",
    "        attention_mask_list.append(bert_input['attention_mask'])\n",
    "        labels.append(label)\n",
    "\n",
    "    # reshape each list to (-1, max_sequence_length)\n",
    "    input_ids_list = torch.tensor(input_ids_list, dtype=torch.int64).reshape(-1, max_sequence_length)\n",
    "    attention_mask_list = torch.tensor(attention_mask_list, dtype=torch.int64).reshape(-1, max_sequence_length)\n",
    "    labels = torch.tensor(labels, dtype=torch.int64).reshape(-1, 1)\n",
    "\n",
    "    return TensorDataset(input_ids_list, attention_mask_list, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "THZAMJKD0qns"
   },
   "source": [
    "## <h1><font color='darkcyan'>1.3 Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "-XMEQPvNnoD1"
   },
   "outputs": [],
   "source": [
    "# train dataset\n",
    "# train dataset\n",
    "from transformers import BertTokenizer , RobertaTokenizer , ElectraTokenizer , DistilBertTokenizer\n",
    "\"\"\"google/electra-base-discriminator\"\"\"\n",
    "\"\"\"google/electra-large-discriminator\"\"\"\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-large\" , do_lower_case=True)\n",
    "\n",
    "batch_size = 128\n",
    "max_sequence_length = 25\n",
    "\n",
    "train_data_encoded = DataLoader(encode_examples(train_dataset, max_sequence_length) , batch_size = batch_size)\n",
    "validation_data_encoded = DataLoader(encode_examples(validation_dataset , max_sequence_length) , batch_size= batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bVN8aYYwDQ2J"
   },
   "source": [
    "## <h1><font color='darkcyan'>1.4 Defining Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9gX3-Etutvd",
    "outputId": "53e4bab8-de14-4bd9-df99-484c607aab48"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification , RobertaForSequenceClassification , XLNetModel , ElectraModel  , BertForSequenceClassification\n",
    "\n",
    "from torch import nn\n",
    "\"\"\"\n",
    "\"roberta-large\"\n",
    "\"\"\"\n",
    "Bert = RobertaForSequenceClassification.from_pretrained(\"roberta-large\", num_labels=len(label_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "oehQs2KWNrEF"
   },
   "outputs": [],
   "source": [
    "model = Bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T9s89YpigfoR"
   },
   "source": [
    "### <h1><font color='darkcyan'>1.4.1 Names and Dimensions of layers \n",
    "The embedding layer,The first of the twelve transformers & The output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n8umCnargQeE",
    "outputId": "fdeba374-0fcb-4751-93fb-655139c55e57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The  model has 393 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "roberta.embeddings.word_embeddings.weight               (50265, 1024)\n",
      "roberta.embeddings.position_embeddings.weight            (514, 1024)\n",
      "roberta.embeddings.token_type_embeddings.weight            (1, 1024)\n",
      "roberta.embeddings.LayerNorm.weight                          (1024,)\n",
      "roberta.embeddings.LayerNorm.bias                            (1024,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "roberta.encoder.layer.0.attention.self.query.weight     (1024, 1024)\n",
      "roberta.encoder.layer.0.attention.self.query.bias            (1024,)\n",
      "roberta.encoder.layer.0.attention.self.key.weight       (1024, 1024)\n",
      "roberta.encoder.layer.0.attention.self.key.bias              (1024,)\n",
      "roberta.encoder.layer.0.attention.self.value.weight     (1024, 1024)\n",
      "roberta.encoder.layer.0.attention.self.value.bias            (1024,)\n",
      "roberta.encoder.layer.0.attention.output.dense.weight   (1024, 1024)\n",
      "roberta.encoder.layer.0.attention.output.dense.bias          (1024,)\n",
      "roberta.encoder.layer.0.attention.output.LayerNorm.weight      (1024,)\n",
      "roberta.encoder.layer.0.attention.output.LayerNorm.bias      (1024,)\n",
      "roberta.encoder.layer.0.intermediate.dense.weight       (4096, 1024)\n",
      "roberta.encoder.layer.0.intermediate.dense.bias              (4096,)\n",
      "roberta.encoder.layer.0.output.dense.weight             (1024, 4096)\n",
      "roberta.encoder.layer.0.output.dense.bias                    (1024,)\n",
      "roberta.encoder.layer.0.output.LayerNorm.weight              (1024,)\n",
      "roberta.encoder.layer.0.output.LayerNorm.bias                (1024,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "classifier.dense.weight                                 (1024, 1024)\n",
      "classifier.dense.bias                                        (1024,)\n",
      "classifier.out_proj.weight                                 (6, 1024)\n",
      "classifier.out_proj.bias                                        (6,)\n"
     ]
    }
   ],
   "source": [
    "# Get all of the model's parameters as a list of tuples.\n",
    "params = list(model.named_parameters())\n",
    "print('The  model has {:} different named parameters.\\n'.format(len(params)))\n",
    "print('==== Embedding Layer ====\\n')\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J_HXgQA3hle6"
   },
   "source": [
    "### <h1><font color='darkcyan'>1.4.2 Function to calculate the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "n-DzdrPNhbjG"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "def f1_score_func(preds, labels):\n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return f1_score(labels_flat, preds_flat, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xYlcR-JTuPFE",
    "outputId": "4989ce07-d5d6-49e2-c49c-5fe8896bce11"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (12): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (13): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (14): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (15): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (16): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (17): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (18): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (19): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (20): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (21): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (22): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (23): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=1024, out_features=6, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tell pytorch to run this model on the GPU.\n",
    "device = 'cuda'\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L2Eyp_VjyQ0u"
   },
   "source": [
    "### <h1><font color='darkcyan'>1.4.3 Helper function for formatting elapsed times.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "bj877kSFyPOB"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "55011udirUHo"
   },
   "source": [
    "## <h1><font color='darkcyan'>1.5 HyperParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "zra00dygu6sH"
   },
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
    "# I believe the 'W' stands for 'Weight Decay fix\"\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 8e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8, # args.adam_epsilon  - default is 1e-8.\n",
    "                  correct_bias = True\n",
    "                )\n",
    "\n",
    "# Total number of training steps is number of batches * number of epochs.\n",
    "total_steps = len(train_data_encoded) * epochs\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 10, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dP5jQsJoBhnm"
   },
   "source": [
    "## <h1><font color='darkcyan'>1.6 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "maeit8DdN7b0",
    "outputId": "8502d59c-f773-40d9-88f6-d9c6666ea20e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 10 ========\n",
      "Training...\n",
      "  Batch    10  of    101.    Elapsed: 0:00:10.\n",
      "  Batch    20  of    101.    Elapsed: 0:00:20.\n",
      "  Batch    30  of    101.    Elapsed: 0:00:29.\n",
      "  Batch    40  of    101.    Elapsed: 0:00:39.\n",
      "  Batch    50  of    101.    Elapsed: 0:00:49.\n",
      "  Batch    60  of    101.    Elapsed: 0:00:59.\n",
      "  Batch    70  of    101.    Elapsed: 0:01:08.\n",
      "  Batch    80  of    101.    Elapsed: 0:01:18.\n",
      "  Batch    90  of    101.    Elapsed: 0:01:28.\n",
      "  Batch   100  of    101.    Elapsed: 0:01:38.\n",
      "\n",
      "  Average training loss: 1.87\n",
      "  Training epcoh took: 0:01:39\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.03\n",
      "  Validation took: 0:00:10\n",
      "\n",
      "======== Epoch 2 / 10 ========\n",
      "Training...\n",
      "  Batch    10  of    101.    Elapsed: 0:00:10.\n",
      "  Batch    20  of    101.    Elapsed: 0:00:20.\n",
      "  Batch    30  of    101.    Elapsed: 0:00:29.\n",
      "  Batch    40  of    101.    Elapsed: 0:00:39.\n",
      "  Batch    50  of    101.    Elapsed: 0:00:49.\n",
      "  Batch    60  of    101.    Elapsed: 0:00:59.\n",
      "  Batch    70  of    101.    Elapsed: 0:01:08.\n",
      "  Batch    80  of    101.    Elapsed: 0:01:18.\n",
      "  Batch    90  of    101.    Elapsed: 0:01:28.\n",
      "  Batch   100  of    101.    Elapsed: 0:01:38.\n",
      "\n",
      "  Average training loss: 0.98\n",
      "  Training epcoh took: 0:01:38\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.88\n",
      "  Validation took: 0:00:10\n",
      "\n",
      "======== Epoch 3 / 10 ========\n",
      "Training...\n",
      "  Batch    10  of    101.    Elapsed: 0:00:10.\n",
      "  Batch    20  of    101.    Elapsed: 0:00:20.\n",
      "  Batch    30  of    101.    Elapsed: 0:00:29.\n",
      "  Batch    40  of    101.    Elapsed: 0:00:39.\n",
      "  Batch    50  of    101.    Elapsed: 0:00:49.\n",
      "  Batch    60  of    101.    Elapsed: 0:00:59.\n",
      "  Batch    70  of    101.    Elapsed: 0:01:08.\n",
      "  Batch    80  of    101.    Elapsed: 0:01:18.\n",
      "  Batch    90  of    101.    Elapsed: 0:01:28.\n",
      "  Batch   100  of    101.    Elapsed: 0:01:38.\n",
      "\n",
      "  Average training loss: 0.29\n",
      "  Training epcoh took: 0:01:38\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.92\n",
      "  Validation took: 0:00:10\n",
      "\n",
      "======== Epoch 4 / 10 ========\n",
      "Training...\n",
      "  Batch    10  of    101.    Elapsed: 0:00:10.\n",
      "  Batch    20  of    101.    Elapsed: 0:00:20.\n",
      "  Batch    30  of    101.    Elapsed: 0:00:29.\n",
      "  Batch    40  of    101.    Elapsed: 0:00:39.\n",
      "  Batch    50  of    101.    Elapsed: 0:00:49.\n",
      "  Batch    60  of    101.    Elapsed: 0:00:59.\n",
      "  Batch    70  of    101.    Elapsed: 0:01:08.\n",
      "  Batch    80  of    101.    Elapsed: 0:01:18.\n",
      "  Batch    90  of    101.    Elapsed: 0:01:28.\n",
      "  Batch   100  of    101.    Elapsed: 0:01:38.\n",
      "\n",
      "  Average training loss: 0.20\n",
      "  Training epcoh took: 0:01:38\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.92\n",
      "  Validation took: 0:00:10\n",
      "\n",
      "======== Epoch 5 / 10 ========\n",
      "Training...\n",
      "  Batch    10  of    101.    Elapsed: 0:00:10.\n",
      "  Batch    20  of    101.    Elapsed: 0:00:20.\n",
      "  Batch    30  of    101.    Elapsed: 0:00:29.\n",
      "  Batch    40  of    101.    Elapsed: 0:00:39.\n",
      "  Batch    50  of    101.    Elapsed: 0:00:49.\n",
      "  Batch    60  of    101.    Elapsed: 0:00:59.\n",
      "  Batch    70  of    101.    Elapsed: 0:01:08.\n",
      "  Batch    80  of    101.    Elapsed: 0:01:18.\n",
      "  Batch    90  of    101.    Elapsed: 0:01:28.\n",
      "  Batch   100  of    101.    Elapsed: 0:01:38.\n",
      "\n",
      "  Average training loss: 0.16\n",
      "  Training epcoh took: 0:01:38\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.93\n",
      "  Validation took: 0:00:10\n",
      "\n",
      "======== Epoch 6 / 10 ========\n",
      "Training...\n",
      "  Batch    10  of    101.    Elapsed: 0:00:10.\n",
      "  Batch    20  of    101.    Elapsed: 0:00:20.\n",
      "  Batch    30  of    101.    Elapsed: 0:00:29.\n",
      "  Batch    40  of    101.    Elapsed: 0:00:39.\n",
      "  Batch    50  of    101.    Elapsed: 0:00:49.\n",
      "  Batch    60  of    101.    Elapsed: 0:00:59.\n",
      "  Batch    70  of    101.    Elapsed: 0:01:08.\n",
      "  Batch    80  of    101.    Elapsed: 0:01:18.\n",
      "  Batch    90  of    101.    Elapsed: 0:01:28.\n",
      "  Batch   100  of    101.    Elapsed: 0:01:38.\n",
      "\n",
      "  Average training loss: 0.15\n",
      "  Training epcoh took: 0:01:38\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.93\n",
      "  Validation took: 0:00:10\n",
      "\n",
      "======== Epoch 7 / 10 ========\n",
      "Training...\n",
      "  Batch    10  of    101.    Elapsed: 0:00:10.\n",
      "  Batch    20  of    101.    Elapsed: 0:00:20.\n",
      "  Batch    30  of    101.    Elapsed: 0:00:29.\n",
      "  Batch    40  of    101.    Elapsed: 0:00:39.\n",
      "  Batch    50  of    101.    Elapsed: 0:00:49.\n",
      "  Batch    60  of    101.    Elapsed: 0:00:59.\n",
      "  Batch    70  of    101.    Elapsed: 0:01:08.\n",
      "  Batch    80  of    101.    Elapsed: 0:01:18.\n",
      "  Batch    90  of    101.    Elapsed: 0:01:28.\n",
      "  Batch   100  of    101.    Elapsed: 0:01:38.\n",
      "\n",
      "  Average training loss: 0.15\n",
      "  Training epcoh took: 0:01:38\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.93\n",
      "  Validation took: 0:00:10\n",
      "\n",
      "======== Epoch 8 / 10 ========\n",
      "Training...\n",
      "  Batch    10  of    101.    Elapsed: 0:00:10.\n",
      "  Batch    20  of    101.    Elapsed: 0:00:20.\n",
      "  Batch    30  of    101.    Elapsed: 0:00:29.\n",
      "  Batch    40  of    101.    Elapsed: 0:00:39.\n",
      "  Batch    50  of    101.    Elapsed: 0:00:49.\n",
      "  Batch    60  of    101.    Elapsed: 0:00:59.\n",
      "  Batch    70  of    101.    Elapsed: 0:01:08.\n",
      "  Batch    80  of    101.    Elapsed: 0:01:18.\n",
      "  Batch    90  of    101.    Elapsed: 0:01:28.\n",
      "  Batch   100  of    101.    Elapsed: 0:01:38.\n",
      "\n",
      "  Average training loss: 0.15\n",
      "  Training epcoh took: 0:01:38\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.93\n",
      "  Validation took: 0:00:10\n",
      "\n",
      "======== Epoch 9 / 10 ========\n",
      "Training...\n",
      "  Batch    10  of    101.    Elapsed: 0:00:10.\n",
      "  Batch    20  of    101.    Elapsed: 0:00:20.\n",
      "  Batch    30  of    101.    Elapsed: 0:00:29.\n",
      "  Batch    40  of    101.    Elapsed: 0:00:39.\n",
      "  Batch    50  of    101.    Elapsed: 0:00:49.\n",
      "  Batch    60  of    101.    Elapsed: 0:00:59.\n",
      "  Batch    70  of    101.    Elapsed: 0:01:08.\n",
      "  Batch    80  of    101.    Elapsed: 0:01:18.\n",
      "  Batch    90  of    101.    Elapsed: 0:01:28.\n",
      "  Batch   100  of    101.    Elapsed: 0:01:38.\n",
      "\n",
      "  Average training loss: 0.15\n",
      "  Training epcoh took: 0:01:38\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.93\n",
      "  Validation took: 0:00:10\n",
      "\n",
      "======== Epoch 10 / 10 ========\n",
      "Training...\n",
      "  Batch    10  of    101.    Elapsed: 0:00:10.\n",
      "  Batch    20  of    101.    Elapsed: 0:00:20.\n",
      "  Batch    30  of    101.    Elapsed: 0:00:29.\n",
      "  Batch    40  of    101.    Elapsed: 0:00:39.\n",
      "  Batch    50  of    101.    Elapsed: 0:00:49.\n",
      "  Batch    60  of    101.    Elapsed: 0:00:59.\n",
      "  Batch    70  of    101.    Elapsed: 0:01:08.\n",
      "  Batch    80  of    101.    Elapsed: 0:01:18.\n",
      "  Batch    90  of    101.    Elapsed: 0:01:28.\n",
      "  Batch   100  of    101.    Elapsed: 0:01:38.\n",
      "\n",
      "  Average training loss: 0.17\n",
      "  Training epcoh took: 0:01:38\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.93\n",
      "  Validation took: 0:00:10\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Multiply accumulation_steps by batch_size to perform gradient accumulation .\n",
    "\n",
    "# Store the average loss after each epoch so we can plot them.\n",
    "loss_values  = []\n",
    "\n",
    "total_loss = 0\n",
    "\n",
    "for i in range(epochs):\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(i + 1, epochs))\n",
    "    print('Training...')\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_loss = 0\n",
    "\n",
    "    # Put the model into training mode. Don't be mislead--the call to \n",
    "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "    # `dropout` and `batchnorm` layers behave differently during training\n",
    "    model.train()\n",
    "\n",
    "    for i_batch, batch in enumerate(train_data_encoded):\n",
    "        # Progress update every 100 batches.\n",
    "        if i_batch % 10 == 0 and not i_batch == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(i_batch, len(train_data_encoded), elapsed))\n",
    "\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        \"\"\"Perform a forward pass (evaluate the model on this training batch)\n",
    "        This will return the loss (rather than the model output) because we\n",
    "        have provided the `labels`.\n",
    "        The documentation for this `model` function is here \"\"\"\n",
    "        outputs = model.forward(input_ids=input_ids, attention_mask=attention_mask , labels = labels)\n",
    "\n",
    "        # outputs = model(input_ids ,  attention_mask)\n",
    "\n",
    "\n",
    "        # The call to `model` always returns a tuple, so we need to pull the \n",
    "        # loss value out of the tuple.\n",
    "        loss = None\n",
    "        labels = labels.float()\n",
    "        loss = outputs[0]\n",
    "\n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "        # single value; the `.item()` function just returns the Python value \n",
    "        # from the tensor.\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) \n",
    "\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "        # modified based on their gradients, the learning rate, etc.                                                 \n",
    "        optimizer.step()                \n",
    "        \n",
    "        model.zero_grad() \n",
    "\n",
    "    # Update the learning rate.                    \n",
    "    scheduler.step()          \n",
    "      \n",
    "    avg_train_loss = total_loss / len(train_data_encoded)\n",
    "    # Store the loss value for plotting the learning curve.\n",
    "    loss_values.append(avg_train_loss)\n",
    "    print('')\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
    "    \n",
    "        # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "    t0 = time.time()\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    model.eval()\n",
    "    # Tracking variables \n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_data_encoded:\n",
    "        \n",
    "        # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        # Telling the model not to compute or store gradients, saving memory and\n",
    "        # speeding up validation\n",
    "        with torch.no_grad():        \n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # This will return the logits rather than the loss because we have\n",
    "            # not provided labels.\n",
    "            # token_type_ids is the same as the \"segment ids\", which \n",
    "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "            # The documentation for this `model` function is here: \n",
    "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "            outputs = model.forward(input_ids=b_input_ids, attention_mask=b_input_mask , labels = b_labels)\n",
    "        \n",
    "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "        # values prior to applying an activation function like the softmax.\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = outputs[1]\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        # Calculate the accuracy for this batch of test sentences.\n",
    "        tmp_eval_accuracy = f1_score_func(logits, label_ids)\n",
    "        \n",
    "        # Accumulate the total accuracy.\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        # Track the number of batches\n",
    "        nb_eval_steps += 1\n",
    "    # Report the final accuracy for this validation run.\n",
    "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RmJNLdcDsKEv"
   },
   "source": [
    "## <h1><font color='darkcyan'>1.7 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "UttMSnFxsNhK"
   },
   "outputs": [],
   "source": [
    "# test dataset\n",
    "batch_size = 128\n",
    "max_sequence_length = 30\n",
    "\n",
    "test_data_encoded = DataLoader(encode_examples(test_dataset, max_sequence_length) , batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "4Dnq_mGMuucG"
   },
   "outputs": [],
   "source": [
    "prediction_inputs = len(test_data_encoded)\n",
    "m = torch.nn.Softmax(dim=1)\n",
    "dict_arg = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "avl3O4IostVr",
    "outputId": "6d6771cb-3813-4ae4-8afa-1db976df26e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels for 34 test sentences...\n",
      "  Accuracy: 0.93\n",
      "DONE.\n"
     ]
    }
   ],
   "source": [
    "# Prediction on test set\n",
    "print('Predicting labels for {:,} test sentences...'.format(prediction_inputs))\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "# Tracking variables \n",
    "predictions , true_labels = [], []\n",
    "# Predict \n",
    "for batch in test_data_encoded:\n",
    "\n",
    "  # Add batch to GPU\n",
    "  batch = tuple(t.to('cuda') for t in batch)\n",
    "  # Unpack the inputs from our dataloader\n",
    "  b_input_ids, b_input_mask, b_labels = batch\n",
    "  # Telling the model not to compute or store gradients, saving memory and \n",
    "  # speeding up prediction\n",
    "  with torch.no_grad():\n",
    "      # Forward pass, calculate logit predictions\n",
    "      outputs = model.forward(input_ids=b_input_ids, attention_mask=b_input_mask , labels = b_labels)\n",
    "  # Move logits and labels to CPU\n",
    "  logits = outputs[1]\n",
    "  logits = logits.detach().cpu().numpy()\n",
    "  \n",
    "  label_ids = b_labels.to('cpu').numpy()\n",
    "  \n",
    "  # Store predictions and true labels\n",
    "\n",
    "  # Calculate the accuracy for this batch of test sentences.\n",
    "  tmp_eval_accuracy = f1_score_func(logits, label_ids)\n",
    "  \n",
    "  # Accumulate the total accuracy.\n",
    "  eval_accuracy += tmp_eval_accuracy\n",
    "  # Track the number of batches\n",
    "  nb_eval_steps += 1\n",
    "    # Report the final accuracy for this validation run.\n",
    "\n",
    "  predictions.append(logits)\n",
    "  true_labels.append(label_ids)\n",
    "print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "predictions = np.concatenate(predictions, axis=0)\n",
    "true_labels = np.concatenate(true_labels, axis=0)\n",
    "\n",
    "print('DONE.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "qFwBeOGiA7wc"
   },
   "outputs": [],
   "source": [
    "def accuracy_per_class(preds, labels):\n",
    "    label_dict_inverse = {v: k for k, v in label_dict.items()}\n",
    "    \n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "\n",
    "    for label in np.unique(labels_flat):\n",
    "        y_preds = preds_flat[labels_flat==label]\n",
    "        y_true = labels_flat[labels_flat==label]\n",
    "        print(f'Class: {label_dict_inverse[label]}')\n",
    "        print(f'Accuracy: {len(y_preds[y_preds==label])}/{len(y_true)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m9yNayN9AKAZ",
    "outputId": "1ed8b01c-8a42-4693-9abc-423fdf035fbc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: sadness\n",
      "Accuracy: 1234/1269\n",
      "\n",
      "Class: anger\n",
      "Accuracy: 567/596\n",
      "\n",
      "Class: love\n",
      "Accuracy: 246/314\n",
      "\n",
      "Class: surprise\n",
      "Accuracy: 126/160\n",
      "\n",
      "Class: fear\n",
      "Accuracy: 471/526\n",
      "\n",
      "Class: happy\n",
      "Accuracy: 1334/1427\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy_per_class(predictions, true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "_j1DKSoPKJTt"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "9gZDoHqvKm4I"
   },
   "outputs": [],
   "source": [
    "predictions = np.argmax(predictions, axis=1).flatten()\n",
    "true_labels = true_labels.flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WEXritFXPBEX",
    "outputId": "5421565d-996d-4bb9-da57-9b3db340b93c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9283743778976329, 0.9268406337371855, 0.9273549980962889, None)"
      ]
     },
     "execution_count": 38,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_recall_fscore_support(predictions, true_labels, average='weighted')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "minYyq_o8dU2"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cf_matrix = confusion_matrix(true_labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "K-6JKmO28dU4",
    "outputId": "b262dfda-e72f-4fb3-df8d-2a672aa86f42"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f2a6f860e90>"
      ]
     },
     "execution_count": 40,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.heatmap(cf_matrix/np.sum(cf_matrix), annot=True, \n",
    "            fmt='.2%', cmap='Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fGYqIh5vThz3"
   },
   "source": [
    "# <h1><font color='darkcyan'>2. Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "ZOeGlkjwTado"
   },
   "outputs": [],
   "source": [
    "data_augmented = pd.read_csv('/content/drive/MyDrive/data even more !!!.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 418
    },
    "id": "jBXFIKlHbBFn",
    "outputId": "3c5867bf-a8aa-4138-8024-c296e93e8632"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Text</th>\n",
       "      <th>Emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29619</th>\n",
       "      <td>5640</td>\n",
       "      <td>2647</td>\n",
       "      <td>melissa stared hesitantly at her friend in dre...</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29620</th>\n",
       "      <td>5641</td>\n",
       "      <td>2648</td>\n",
       "      <td>successive state representation elections have...</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29621</th>\n",
       "      <td>5642</td>\n",
       "      <td>2649</td>\n",
       "      <td>vincent was irritated but yet not dismay</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29622</th>\n",
       "      <td>5643</td>\n",
       "      <td>2650</td>\n",
       "      <td>kendall - man hume turned back and to complete...</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29623</th>\n",
       "      <td>5644</td>\n",
       "      <td>2651</td>\n",
       "      <td>inside i am all dismayed, disappointed but not...</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29624 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  ...  Emotion\n",
       "0               0  ...  sadness\n",
       "1               1  ...  sadness\n",
       "2               2  ...    anger\n",
       "3               3  ...     love\n",
       "4               4  ...    anger\n",
       "...           ...  ...      ...\n",
       "29619        5640  ...     fear\n",
       "29620        5641  ...     fear\n",
       "29621        5642  ...     fear\n",
       "29622        5643  ...     fear\n",
       "29623        5644  ...     fear\n",
       "\n",
       "[29624 rows x 4 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_augmented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yXUakK8uqUKy"
   },
   "source": [
    "## 2.1 removing punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "v-IQhhr1qUK1",
    "outputId": "f4ae0010-c05c-4cf7-9663-0561999acd6d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 43,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "fMfoUBr6qUK2",
    "outputId": "58924e2b-d2bb-449f-a5af-8c8f0b839816"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Text</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>text_wo_punct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>sadness</td>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>anger</td>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>love</td>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>anger</td>\n",
       "      <td>i am feeling grouchy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  ...                                      text_wo_punct\n",
       "0           0  ...                            i didnt feel humiliated\n",
       "1           1  ...  i can go from feeling so hopeless to so damned...\n",
       "2           2  ...   im grabbing a minute to post i feel greedy wrong\n",
       "3           3  ...  i am ever feeling nostalgic about the fireplac...\n",
       "4           4  ...                               i am feeling grouchy\n",
       "\n",
       "[5 rows x 5 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_punctuation(text):\n",
    "    no_punct=[words for words in text if words not in string.punctuation]\n",
    "    words_wo_punct=''.join(no_punct)\n",
    "    return words_wo_punct\n",
    "data_augmented['text_wo_punct']=data_augmented['Text'].apply(lambda x: remove_punctuation(x))\n",
    "data_augmented.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KMX06e4AqUK2"
   },
   "source": [
    "## 2.2 tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "HoHzJS3pqUK2",
    "outputId": "3978196a-d31b-4142-93c6-b7f254c19b90"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Text</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>text_wo_punct</th>\n",
       "      <th>text_wo_punct_split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>sadness</td>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>[i, didnt, feel, humiliated]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>[i, can, go, from, feeling, so, hopeless, to, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>anger</td>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>[im, grabbing, a, minute, to, post, i, feel, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>love</td>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>[i, am, ever, feeling, nostalgic, about, the, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>anger</td>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>[i, am, feeling, grouchy]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  ...                                text_wo_punct_split\n",
       "0           0  ...                       [i, didnt, feel, humiliated]\n",
       "1           1  ...  [i, can, go, from, feeling, so, hopeless, to, ...\n",
       "2           2  ...  [im, grabbing, a, minute, to, post, i, feel, g...\n",
       "3           3  ...  [i, am, ever, feeling, nostalgic, about, the, ...\n",
       "4           4  ...                          [i, am, feeling, grouchy]\n",
       "\n",
       "[5 rows x 6 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(text):\n",
    "    split=re.split(\"\\W+\",text) \n",
    "    return split\n",
    "data_augmented['text_wo_punct_split']=data_augmented['text_wo_punct'].apply(lambda x: tokenize(x.lower()))\n",
    "data_augmented.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 324
    },
    "id": "PyKkNcp4qUK2",
    "outputId": "54cf2d52-88ee-4f70-8c27-71e9daebdd50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\"]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Text</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>text_wo_punct</th>\n",
       "      <th>text_wo_punct_split</th>\n",
       "      <th>text_wo_punct_split_wo_stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>sadness</td>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>[i, didnt, feel, humiliated]</td>\n",
       "      <td>[didnt, feel, humiliated]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>[i, can, go, from, feeling, so, hopeless, to, ...</td>\n",
       "      <td>[go, feeling, hopeless, damned, hopeful, aroun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>anger</td>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>[im, grabbing, a, minute, to, post, i, feel, g...</td>\n",
       "      <td>[im, grabbing, minute, post, feel, greedy, wrong]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>love</td>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>[i, am, ever, feeling, nostalgic, about, the, ...</td>\n",
       "      <td>[ever, feeling, nostalgic, fireplace, know, st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>anger</td>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>[i, am, feeling, grouchy]</td>\n",
       "      <td>[feeling, grouchy]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  ...                   text_wo_punct_split_wo_stopwords\n",
       "0           0  ...                          [didnt, feel, humiliated]\n",
       "1           1  ...  [go, feeling, hopeless, damned, hopeful, aroun...\n",
       "2           2  ...  [im, grabbing, minute, post, feel, greedy, wrong]\n",
       "3           3  ...  [ever, feeling, nostalgic, fireplace, know, st...\n",
       "4           4  ...                                 [feeling, grouchy]\n",
       "\n",
       "[5 rows x 7 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stopword = nltk.corpus.stopwords.words('english')\n",
    "print(stopword[:11])\n",
    "def remove_stopwords(text):\n",
    "    text=[word for word in text if word not in stopword]\n",
    "    return text\n",
    "data_augmented['text_wo_punct_split_wo_stopwords'] = data_augmented['text_wo_punct_split'].apply(lambda x: remove_stopwords(x))\n",
    "data_augmented.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0od9eDl5qUK3"
   },
   "source": [
    "## 2.3 lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XrJFtERiqUK3",
    "outputId": "06836a94-1667-4928-e0a6-312c71f06a3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# import these modules\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(w) for w in text]  ##Notice the use of text.\n",
    "\n",
    "def join_lem(text):\n",
    "  return ' '.join(w for w in text)\n",
    "\n",
    "data_augmented['Text'] = data_augmented[\"text_wo_punct_split_wo_stopwords\"].apply(lemmatize_text)\n",
    "data_augmented['Text'] = data_augmented['Text'].apply(join_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "WziExxl4qUK4"
   },
   "outputs": [],
   "source": [
    "data_augmented = data_augmented.filter(['Text','Emotion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 418
    },
    "id": "8g6D3Cb4qUK4",
    "outputId": "c06cb2bd-6c52-4926-dd04-9fb3cf88090b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>didnt feel humiliated</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>go feeling hopeless damned hopeful around some...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing minute post feel greedy wrong</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ever feeling nostalgic fireplace know still pr...</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>feeling grouchy</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29619</th>\n",
       "      <td>melissa stared hesitantly friend dread dism</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29620</th>\n",
       "      <td>successive state representation election seen ...</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29621</th>\n",
       "      <td>vincent irritated yet dismay</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29622</th>\n",
       "      <td>kendall man hume turned back completely face d...</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29623</th>\n",
       "      <td>inside dismayed disappointed surpris</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29624 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text  Emotion\n",
       "0                                  didnt feel humiliated  sadness\n",
       "1      go feeling hopeless damned hopeful around some...  sadness\n",
       "2              im grabbing minute post feel greedy wrong    anger\n",
       "3      ever feeling nostalgic fireplace know still pr...     love\n",
       "4                                        feeling grouchy    anger\n",
       "...                                                  ...      ...\n",
       "29619        melissa stared hesitantly friend dread dism     fear\n",
       "29620  successive state representation election seen ...     fear\n",
       "29621                       vincent irritated yet dismay     fear\n",
       "29622  kendall man hume turned back completely face d...     fear\n",
       "29623               inside dismayed disappointed surpris     fear\n",
       "\n",
       "[29624 rows x 2 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xrffwo30T5O0",
    "outputId": "35f44e0f-8b64-4afc-c302-6c04d8c6d98f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'anger': 1, 'fear': 4, 'happy': 5, 'love': 2, 'sadness': 0, 'surprise': 3}"
      ]
     },
     "execution_count": 50,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "possible_labels = data_augmented.Emotion.unique()\n",
    "\n",
    "label_dict = {}\n",
    "for index, possible_label in enumerate(possible_labels):\n",
    "    label_dict[possible_label] = index\n",
    "label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 418
    },
    "id": "yvrswCrpT-mi",
    "outputId": "e22d8bcf-0812-444b-97e5-19c8a781ac7c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>didnt feel humiliated</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>go feeling hopeless damned hopeful around some...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing minute post feel greedy wrong</td>\n",
       "      <td>anger</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ever feeling nostalgic fireplace know still pr...</td>\n",
       "      <td>love</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>feeling grouchy</td>\n",
       "      <td>anger</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29619</th>\n",
       "      <td>melissa stared hesitantly friend dread dism</td>\n",
       "      <td>fear</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29620</th>\n",
       "      <td>successive state representation election seen ...</td>\n",
       "      <td>fear</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29621</th>\n",
       "      <td>vincent irritated yet dismay</td>\n",
       "      <td>fear</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29622</th>\n",
       "      <td>kendall man hume turned back completely face d...</td>\n",
       "      <td>fear</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29623</th>\n",
       "      <td>inside dismayed disappointed surpris</td>\n",
       "      <td>fear</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29624 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text  Emotion  label\n",
       "0                                  didnt feel humiliated  sadness      0\n",
       "1      go feeling hopeless damned hopeful around some...  sadness      0\n",
       "2              im grabbing minute post feel greedy wrong    anger      1\n",
       "3      ever feeling nostalgic fireplace know still pr...     love      2\n",
       "4                                        feeling grouchy    anger      1\n",
       "...                                                  ...      ...    ...\n",
       "29619        melissa stared hesitantly friend dread dism     fear      4\n",
       "29620  successive state representation election seen ...     fear      4\n",
       "29621                       vincent irritated yet dismay     fear      4\n",
       "29622  kendall man hume turned back completely face d...     fear      4\n",
       "29623               inside dismayed disappointed surpris     fear      4\n",
       "\n",
       "[29624 rows x 3 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_augmented['label'] = data_augmented.Emotion.replace(label_dict)\n",
    "data_augmented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5E-5cc56XpRs"
   },
   "source": [
    "## <h1><font color=darkmagneta>2.4 Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 352
    },
    "id": "CZUAwwtLXpRs",
    "outputId": "1a44e986-83c1-4260-c6bf-0e136cf18cb8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f26c3a29a10>"
      ]
     },
     "execution_count": 52,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(data_augmented['Emotion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "evin1y1OUYOt"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_aug, valid_test_aug = train_test_split(data_augmented, test_size=0.4 , shuffle=True)\n",
    "\n",
    "validate_aug , test_aug = train_test_split(valid_test_aug, test_size=0.5 , shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "5qGrmBHfUYOw"
   },
   "outputs": [],
   "source": [
    "# CONVERT TO TensorFlow DATASETS\n",
    "train_dataset_aug = convert_to_TF(train_aug)\n",
    "\n",
    "validation_dataset_aug = convert_to_TF(validate_aug)\n",
    "\n",
    "test_dataset_aug = convert_to_TF(test_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "N3FBWj-hUYOw"
   },
   "outputs": [],
   "source": [
    "# train dataset\n",
    "# train dataset\n",
    "from transformers import BertTokenizer , RobertaTokenizer , ElectraTokenizer , DistilBertTokenizer\n",
    "\"\"\"google/electra-base-discriminator\"\"\"\n",
    "\"\"\"google/electra-large-discriminator\"\"\"\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-large\" , do_lower_case=True)\n",
    "\n",
    "batch_size = 128\n",
    "max_sequence_length = 25\n",
    "\n",
    "train_data_encoded_aug = DataLoader(encode_examples(train_dataset_aug, max_sequence_length) , batch_size = batch_size)\n",
    "validation_data_encoded_aug = DataLoader(encode_examples(validation_dataset_aug , max_sequence_length) , batch_size= batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tI0uG6IEV0id"
   },
   "source": [
    "## <h1><font color='darkcyan'>2.5 HyperParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YvMroieTNGun",
    "outputId": "0ed4a74a-4828-4c2d-b526-b69bf0d9e0e3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (12): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (13): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (14): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (15): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (16): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (17): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (18): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (19): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (20): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (21): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (22): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (23): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=1024, out_features=6, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_aug = Bert\n",
    "model_aug.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "PZPh6wNDV0il"
   },
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
    "# I believe the 'W' stands for 'Weight Decay fix\"\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "optimizer = AdamW(model_aug.parameters(),\n",
    "                  lr = 8e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8, # args.adam_epsilon  - default is 1e-8.\n",
    "                  correct_bias = True\n",
    "                )\n",
    "\n",
    "# Total number of training steps is number of batches * number of epochs.\n",
    "total_steps = len(train_data_encoded_aug) * epochs\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 10, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WS65xQsQUYOx",
    "outputId": "b0150b6e-765e-4172-f15b-0a6f16d25c58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 10 ========\n",
      "Training...\n",
      "  Batch   100  of    139.    Elapsed: 0:01:38.\n",
      "\n",
      "  Average training loss: 0.25\n",
      "  Training epcoh took: 0:02:16\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.91\n",
      "  Validation took: 0:00:14\n",
      "\n",
      "======== Epoch 2 / 10 ========\n",
      "Training...\n",
      "  Batch   100  of    139.    Elapsed: 0:01:38.\n",
      "\n",
      "  Average training loss: 0.19\n",
      "  Training epcoh took: 0:02:16\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.93\n",
      "  Validation took: 0:00:14\n",
      "\n",
      "======== Epoch 3 / 10 ========\n",
      "Training...\n",
      "  Batch   100  of    139.    Elapsed: 0:01:38.\n",
      "\n",
      "  Average training loss: 0.15\n",
      "  Training epcoh took: 0:02:16\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.94\n",
      "  Validation took: 0:00:14\n",
      "\n",
      "======== Epoch 4 / 10 ========\n",
      "Training...\n",
      "  Batch   100  of    139.    Elapsed: 0:01:38.\n",
      "\n",
      "  Average training loss: 0.13\n",
      "  Training epcoh took: 0:02:16\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.93\n",
      "  Validation took: 0:00:14\n",
      "\n",
      "======== Epoch 5 / 10 ========\n",
      "Training...\n",
      "  Batch   100  of    139.    Elapsed: 0:01:38.\n",
      "\n",
      "  Average training loss: 0.12\n",
      "  Training epcoh took: 0:02:16\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.93\n",
      "  Validation took: 0:00:14\n",
      "\n",
      "======== Epoch 6 / 10 ========\n",
      "Training...\n",
      "  Batch   100  of    139.    Elapsed: 0:01:38.\n",
      "\n",
      "  Average training loss: 0.12\n",
      "  Training epcoh took: 0:02:16\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.93\n",
      "  Validation took: 0:00:14\n",
      "\n",
      "======== Epoch 7 / 10 ========\n",
      "Training...\n",
      "  Batch   100  of    139.    Elapsed: 0:01:38.\n",
      "\n",
      "  Average training loss: 0.12\n",
      "  Training epcoh took: 0:02:16\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.93\n",
      "  Validation took: 0:00:14\n",
      "\n",
      "======== Epoch 8 / 10 ========\n",
      "Training...\n",
      "  Batch   100  of    139.    Elapsed: 0:01:38.\n",
      "\n",
      "  Average training loss: 0.12\n",
      "  Training epcoh took: 0:02:16\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.93\n",
      "  Validation took: 0:00:14\n",
      "\n",
      "======== Epoch 9 / 10 ========\n",
      "Training...\n",
      "  Batch   100  of    139.    Elapsed: 0:01:38.\n",
      "\n",
      "  Average training loss: 0.12\n",
      "  Training epcoh took: 0:02:16\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.90\n",
      "  Validation took: 0:00:14\n",
      "\n",
      "======== Epoch 10 / 10 ========\n",
      "Training...\n",
      "  Batch   100  of    139.    Elapsed: 0:01:38.\n",
      "\n",
      "  Average training loss: 0.13\n",
      "  Training epcoh took: 0:02:16\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.93\n",
      "  Validation took: 0:00:14\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Multiply accumulation_steps by batch_size to perform gradient accumulation .\n",
    "\n",
    "# Store the average loss after each epoch so we can plot them.\n",
    "loss_values  = []\n",
    "\n",
    "total_loss = 0\n",
    "\n",
    "for i in range(epochs):\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(i + 1, epochs))\n",
    "    print('Training...')\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_loss = 0\n",
    "\n",
    "    # Put the model into training mode. Don't be mislead--the call to \n",
    "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "    # `dropout` and `batchnorm` layers behave differently during training\n",
    "    model_aug.train()\n",
    "\n",
    "    for i_batch, batch in enumerate(train_data_encoded_aug):\n",
    "        # Progress update every 100 batches.\n",
    "        if i_batch % 100 == 0 and not i_batch == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(i_batch, len(train_data_encoded_aug), elapsed))\n",
    "\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        \"\"\"Perform a forward pass (evaluate the model on this training batch)\n",
    "        This will return the loss (rather than the model output) because we\n",
    "        have provided the `labels`.\n",
    "        The documentation for this `model` function is here \"\"\"\n",
    "        outputs = model_aug.forward(input_ids=input_ids, attention_mask=attention_mask , labels = labels)\n",
    "\n",
    "        # outputs = model(input_ids ,  attention_mask)\n",
    "\n",
    "\n",
    "        # The call to `model` always returns a tuple, so we need to pull the \n",
    "        # loss value out of the tuple.\n",
    "        loss = None\n",
    "        loss_fct = torch.nn.BCELoss()\n",
    "        labels = labels.float()\n",
    "        loss = outputs[0]\n",
    "\n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "        # single value; the `.item()` function just returns the Python value \n",
    "        # from the tensor.\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) \n",
    "\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "        # modified based on their gradients, the learning rate, etc.                                                 \n",
    "        optimizer.step()                \n",
    "        \n",
    "        model_aug.zero_grad() \n",
    "\n",
    "    # Update the learning rate.                    \n",
    "    scheduler.step()          \n",
    "      \n",
    "    avg_train_loss = total_loss / len(train_data_encoded_aug)\n",
    "    # Store the loss value for plotting the learning curve.\n",
    "    loss_values.append(avg_train_loss)\n",
    "    print('')\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
    "    \n",
    "        # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "    t0 = time.time()\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    model_aug.eval()\n",
    "    # Tracking variables \n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_data_encoded_aug:\n",
    "        \n",
    "        # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        # Telling the model not to compute or store gradients, saving memory and\n",
    "        # speeding up validation\n",
    "        with torch.no_grad():        \n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # This will return the logits rather than the loss because we have\n",
    "            # not provided labels.\n",
    "            # token_type_ids is the same as the \"segment ids\", which \n",
    "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "            # The documentation for this `model` function is here: \n",
    "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "            outputs = model_aug.forward(input_ids=b_input_ids, attention_mask=b_input_mask , labels = b_labels)\n",
    "        \n",
    "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "        # values prior to applying an activation function like the softmax.\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = outputs[1]\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        # Calculate the accuracy for this batch of test sentences.\n",
    "        tmp_eval_accuracy = f1_score_func(logits, label_ids)\n",
    "        \n",
    "        # Accumulate the total accuracy.\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        # Track the number of batches\n",
    "        nb_eval_steps += 1\n",
    "    # Report the final accuracy for this validation run.\n",
    "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eB1ZUkRyUYOx"
   },
   "source": [
    "## <h1><font color='darkcyan'>2.6 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "F8VR5h7pUYOx"
   },
   "outputs": [],
   "source": [
    "# test dataset\n",
    "batch_size = 128\n",
    "max_sequence_length = 60\n",
    "\n",
    "test_data_encoded_aug = DataLoader(encode_examples(test_dataset_aug, max_sequence_length) , batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "eiqGvAZnUYOy"
   },
   "outputs": [],
   "source": [
    "prediction_inputs = len(test_data_encoded_aug)\n",
    "m = torch.nn.Softmax(dim=1)\n",
    "dict_arg = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SrOaPr9pUYOy",
    "outputId": "a1ff213f-f39f-4f68-aa9f-536bbf166e8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels for 47 test sentences...\n",
      "  Accuracy: 0.94\n",
      "DONE.\n"
     ]
    }
   ],
   "source": [
    "# Prediction on test set\n",
    "print('Predicting labels for {:,} test sentences...'.format(prediction_inputs))\n",
    "# Put model in evaluation mode\n",
    "model_aug.eval()\n",
    "# Tracking variables \n",
    "predictions , true_labels = [], []\n",
    "# Predict \n",
    "for batch in test_data_encoded_aug:\n",
    "\n",
    "  # Add batch to GPU\n",
    "  batch = tuple(t.to('cuda') for t in batch)\n",
    "  # Unpack the inputs from our dataloader\n",
    "  b_input_ids, b_input_mask, b_labels = batch\n",
    "  # Telling the model not to compute or store gradients, saving memory and \n",
    "  # speeding up prediction\n",
    "  with torch.no_grad():\n",
    "      # Forward pass, calculate logit predictions\n",
    "      outputs = model_aug.forward(input_ids=b_input_ids, attention_mask=b_input_mask , labels = b_labels)\n",
    "  # Move logits and labels to CPU\n",
    "  logits = outputs[1]\n",
    "  logits = logits.detach().cpu().numpy()\n",
    "  \n",
    "  label_ids = b_labels.to('cpu').numpy()\n",
    "  \n",
    "  # Store predictions and true labels\n",
    "\n",
    "  # Calculate the accuracy for this batch of test sentences.\n",
    "  tmp_eval_accuracy = f1_score_func(logits, label_ids)\n",
    "  \n",
    "  # Accumulate the total accuracy.\n",
    "  eval_accuracy += tmp_eval_accuracy\n",
    "  # Track the number of batches\n",
    "  nb_eval_steps += 1\n",
    "    # Report the final accuracy for this validation run.\n",
    "\n",
    "  predictions.append(logits)\n",
    "  true_labels.append(label_ids)\n",
    "print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "predictions = np.concatenate(predictions, axis=0)\n",
    "true_labels = np.concatenate(true_labels, axis=0)\n",
    "\n",
    "print('DONE.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FWNpWhAPUYOy",
    "outputId": "4e55cba9-fe35-4670-cc16-35d4573e621a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: sadness\n",
      "Accuracy: 1207/1261\n",
      "\n",
      "Class: anger\n",
      "Accuracy: 1195/1219\n",
      "\n",
      "Class: love\n",
      "Accuracy: 645/669\n",
      "\n",
      "Class: surprise\n",
      "Accuracy: 356/366\n",
      "\n",
      "Class: fear\n",
      "Accuracy: 874/994\n",
      "\n",
      "Class: happy\n",
      "Accuracy: 1298/1416\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy_per_class(predictions, true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "ptYVxFV0UYOz"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "BeN-sNmwUYOz"
   },
   "outputs": [],
   "source": [
    "predictions = np.argmax(predictions, axis=1).flatten()\n",
    "true_labels = true_labels.flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YqlYGEmcUYOz",
    "outputId": "09eb084d-c96b-468d-8d34-2f5143f5b118"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9433303533718421, 0.9409282700421941, 0.9401424796645732, None)"
      ]
     },
     "execution_count": 65,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_recall_fscore_support(predictions, true_labels, average='weighted')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "I2WjszhN0t7i"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cf_matrix = confusion_matrix(true_labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "egQ136eIz5vp",
    "outputId": "57c77be1-3c58-412c-a274-17af8ed6a8a4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f29f8cfaed0>"
      ]
     },
     "execution_count": 67,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.heatmap(cf_matrix/np.sum(cf_matrix), annot=True, \n",
    "            fmt='.2%', cmap='Blues')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Emotion classification RoBERTa.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
