{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wkhv8AbK3tH8"
   },
   "source": [
    "\n",
    "#### import libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "TY1C95z5TIGT"
   },
   "outputs": [],
   "source": [
    "import torch #import torch library\n",
    "import torch.nn as nn #import torch neural network library\n",
    "import torch.nn.functional as F #import functional neural network module\n",
    "import torch.optim as optim #import optimizer neural network module\n",
    "from torch.autograd import Variable #import variable that connect to automatic differentiation\n",
    "from torchvision import datasets, transforms #import torchvision for datasets and transform\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0q0pkpf-4J-1"
   },
   "source": [
    "### variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "uhL5pgrpn-t7"
   },
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 100\n",
    "learning_rate=0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gC_0IkcH4X7v"
   },
   "source": [
    "\n",
    "#### DNN function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "fSr3jidvUjX5"
   },
   "outputs": [],
   "source": [
    "class DNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DNN, self).__init__() #load super class for training data\n",
    "        self.fc1 = nn.Linear(784, 600)\n",
    "        self.fc2 = nn.Linear(600, 450)\n",
    "        self.fc3 = nn.Linear(450, 200)\n",
    "        self.fc4 = nn.Linear(200, 100)\n",
    "        self.fc5 = nn.Linear(100, 50)\n",
    "        self.fc6 = nn.Linear(50, 20)\n",
    "        self.fc7 = nn.Linear(20, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self, x): #feed forward\n",
    "        layer1 = x.view(-1, 784) #make it flat from 0 - 320\n",
    "        layer2 = self.relu(self.fc1(layer1)) #layer2 = layer1 -> fc1 -> relu\n",
    "        layer3 = self.relu(self.fc2(layer2)) #layer3 = layer2 -> fc2 -> relu\n",
    "        layer4 = self.relu(self.fc3(layer3)) #layer4 = layer3 -> fc3 -> relu\n",
    "        layer5 = self.relu(self.fc3(layer3)) #layer5 = layer4 -> fc5 -> relu\n",
    "        layer6 = self.relu(self.fc3(layer3)) #layer6 = layer5 -> fc6 -> relu\n",
    "        layer7 = self.relu(self.fc3(layer3)) #layer7 = layer6 -> fc7 -> relu\n",
    "        return F.log_softmax(layer7, -1) #softmax activation to layer4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6d5csWRe4fFs"
   },
   "source": [
    "#### load MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "1YnkVvFiU_Dm"
   },
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def read(self):\n",
    "        learning_rate = 0.001 \n",
    "        train_dataset = torchvision.datasets.MNIST(root='./data',train=True,transform=transforms.ToTensor(),download=True)\n",
    "        validation_dataset = torchvision.datasets.MNIST(root='./data',train=False,transform=transforms.ToTensor())\n",
    "        train_loader = torch.utils.data.DataLoader(dataset=train_dataset,batch_size=batch_size,shuffle=True)\n",
    "        validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset,batch_size=batch_size,shuffle=False)\n",
    "        return train_loader, validation_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yf4rZIUcmGW2",
    "outputId": "bfbc6e32-bfe0-40de-8a63-1922f06a2125"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c56f446ed3e7469e8d71b5e0c09d3dfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9912422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8281467d27574a7aab6ba10cde943889",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45eb6aadce9b445bbcfb694f5f0e1176",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1648877 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1688a4d4bfa4b8eb63c3fc614519312",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4542 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "600\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "train_loader,validation_loader = Dataset().read()\n",
    "print(len(train_loader))\n",
    "print(len(validation_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jDawsr2D4woh"
   },
   "source": [
    "#### create model and choose optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "lENhRT6M4s7q"
   },
   "outputs": [],
   "source": [
    "model = DNN()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z6lY3kx64vpq"
   },
   "source": [
    "#### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DWFdVGCAVCC5",
    "outputId": "c94f2e6e-7413-4aa8-f836-f1b26ccfa3e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epochs: 0, Loss: 5.303627 \n",
      "Train Epochs: 0, Loss: 2.963827 \n",
      "Train Epochs: 0, Loss: 2.539569 \n",
      "Train Epochs: 0, Loss: 2.717967 \n",
      "Train Epochs: 0, Loss: 2.184889 \n",
      "Train Epochs: 0, Loss: 2.694599 \n",
      "Train Epochs: 0, Loss: 2.436944 \n",
      "Train Epochs: 0, Loss: 2.186560 \n",
      "Train Epochs: 0, Loss: 2.582582 \n",
      "Train Epochs: 0, Loss: 2.628036 \n",
      "Train Epochs: 0, Loss: 2.323488 \n",
      "Train Epochs: 0, Loss: 2.125279 \n",
      "Train Epochs: 0, Loss: 2.306149 \n",
      "Train Epochs: 0, Loss: 2.360191 \n",
      "Train Epochs: 0, Loss: 2.102025 \n",
      "Train Epochs: 0, Loss: 2.451490 \n",
      "Train Epochs: 0, Loss: 2.374441 \n",
      "Train Epochs: 0, Loss: 2.285699 \n",
      "Train Epochs: 0, Loss: 2.451483 \n",
      "Train Epochs: 0, Loss: 1.720210 \n",
      "Train Epochs: 0, Loss: 2.483444 \n",
      "Train Epochs: 0, Loss: 1.990314 \n",
      "Train Epochs: 0, Loss: 2.452683 \n",
      "Train Epochs: 0, Loss: 2.642159 \n",
      "Train Epochs: 0, Loss: 2.675135 \n",
      "Train Epochs: 0, Loss: 2.402518 \n",
      "Train Epochs: 0, Loss: 2.483387 \n",
      "Train Epochs: 0, Loss: 2.197013 \n",
      "Train Epochs: 0, Loss: 2.102396 \n",
      "Train Epochs: 0, Loss: 2.369204 \n",
      "Train Epochs: 0, Loss: 2.440348 \n",
      "Train Epochs: 0, Loss: 2.380169 \n",
      "Train Epochs: 0, Loss: 1.772021 \n",
      "Train Epochs: 0, Loss: 2.676910 \n",
      "Train Epochs: 0, Loss: 1.973879 \n",
      "Train Epochs: 0, Loss: 2.807587 \n",
      "Train Epochs: 0, Loss: 2.628740 \n",
      "Train Epochs: 0, Loss: 2.450475 \n",
      "Train Epochs: 0, Loss: 2.469480 \n",
      "Train Epochs: 0, Loss: 1.901190 \n",
      "Train Epochs: 0, Loss: 2.325128 \n",
      "Train Epochs: 0, Loss: 2.038065 \n",
      "Train Epochs: 0, Loss: 1.651097 \n",
      "Train Epochs: 0, Loss: 2.407472 \n",
      "Train Epochs: 0, Loss: 2.205633 \n",
      "Train Epochs: 0, Loss: 2.310686 \n",
      "Train Epochs: 0, Loss: 2.512162 \n",
      "Train Epochs: 0, Loss: 2.502402 \n",
      "Train Epochs: 0, Loss: 2.425418 \n",
      "Train Epochs: 0, Loss: 2.403437 \n",
      "Train Epochs: 0, Loss: 2.022476 \n",
      "Train Epochs: 0, Loss: 1.884174 \n",
      "Train Epochs: 0, Loss: 2.122367 \n",
      "Train Epochs: 0, Loss: 2.329757 \n",
      "Train Epochs: 0, Loss: 2.611974 \n",
      "Train Epochs: 0, Loss: 1.917902 \n",
      "Train Epochs: 0, Loss: 2.322356 \n",
      "Train Epochs: 0, Loss: 2.073133 \n",
      "Train Epochs: 0, Loss: 2.002162 \n",
      "Train Epochs: 0, Loss: 2.282721 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DEFAUL~1.DES\\AppData\\Local\\Temp/ipykernel_11628/3209282462.py:16: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  data, label = Variable(data,volatile=True), Variable(label) #create torch variable and enter data and label into it\n",
      "C:\\Users\\Default.DESKTOP-H4O1L0D\\anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Loss: 2.2913, Accuracy: 58\n",
      "Train Epochs: 1, Loss: 2.071299 \n",
      "Train Epochs: 1, Loss: 2.617785 \n",
      "Train Epochs: 1, Loss: 2.988955 \n",
      "Train Epochs: 1, Loss: 2.085460 \n",
      "Train Epochs: 1, Loss: 2.384488 \n",
      "Train Epochs: 1, Loss: 1.998793 \n",
      "Train Epochs: 1, Loss: 2.256792 \n",
      "Train Epochs: 1, Loss: 2.305472 \n",
      "Train Epochs: 1, Loss: 2.039130 \n",
      "Train Epochs: 1, Loss: 2.257450 \n",
      "Train Epochs: 1, Loss: 2.320605 \n",
      "Train Epochs: 1, Loss: 2.203342 \n",
      "Train Epochs: 1, Loss: 2.125736 \n",
      "Train Epochs: 1, Loss: 2.060676 \n",
      "Train Epochs: 1, Loss: 2.148826 \n",
      "Train Epochs: 1, Loss: 2.071496 \n",
      "Train Epochs: 1, Loss: 2.551202 \n",
      "Train Epochs: 1, Loss: 2.354902 \n",
      "Train Epochs: 1, Loss: 2.131413 \n",
      "Train Epochs: 1, Loss: 2.203523 \n",
      "Train Epochs: 1, Loss: 2.427907 \n",
      "Train Epochs: 1, Loss: 2.271388 \n",
      "Train Epochs: 1, Loss: 2.022129 \n",
      "Train Epochs: 1, Loss: 2.092935 \n",
      "Train Epochs: 1, Loss: 2.666684 \n",
      "Train Epochs: 1, Loss: 2.133558 \n",
      "Train Epochs: 1, Loss: 2.569282 \n",
      "Train Epochs: 1, Loss: 2.339043 \n",
      "Train Epochs: 1, Loss: 2.359807 \n",
      "Train Epochs: 1, Loss: 2.531638 \n",
      "Train Epochs: 1, Loss: 1.986125 \n",
      "Train Epochs: 1, Loss: 1.801268 \n",
      "Train Epochs: 1, Loss: 2.200491 \n",
      "Train Epochs: 1, Loss: 2.286961 \n",
      "Train Epochs: 1, Loss: 2.084827 \n",
      "Train Epochs: 1, Loss: 1.816576 \n",
      "Train Epochs: 1, Loss: 2.064818 \n",
      "Train Epochs: 1, Loss: 2.083736 \n",
      "Train Epochs: 1, Loss: 2.132454 \n",
      "Train Epochs: 1, Loss: 2.164109 \n",
      "Train Epochs: 1, Loss: 2.052219 \n",
      "Train Epochs: 1, Loss: 2.433326 \n",
      "Train Epochs: 1, Loss: 2.210441 \n",
      "Train Epochs: 1, Loss: 2.142792 \n",
      "Train Epochs: 1, Loss: 2.191158 \n",
      "Train Epochs: 1, Loss: 2.926113 \n",
      "Train Epochs: 1, Loss: 2.439478 \n",
      "Train Epochs: 1, Loss: 1.993060 \n",
      "Train Epochs: 1, Loss: 2.013845 \n",
      "Train Epochs: 1, Loss: 2.359294 \n",
      "Train Epochs: 1, Loss: 2.067840 \n",
      "Train Epochs: 1, Loss: 2.236278 \n",
      "Train Epochs: 1, Loss: 2.202634 \n",
      "Train Epochs: 1, Loss: 2.316202 \n",
      "Train Epochs: 1, Loss: 2.004487 \n",
      "Train Epochs: 1, Loss: 2.370048 \n",
      "Train Epochs: 1, Loss: 2.135686 \n",
      "Train Epochs: 1, Loss: 2.148646 \n",
      "Train Epochs: 1, Loss: 2.319347 \n",
      "Train Epochs: 1, Loss: 2.571256 \n",
      "\n",
      "Average Loss: 2.2663, Accuracy: 58\n",
      "Train Epochs: 2, Loss: 2.212527 \n",
      "Train Epochs: 2, Loss: 2.248154 \n",
      "Train Epochs: 2, Loss: 2.261002 \n",
      "Train Epochs: 2, Loss: 2.702979 \n",
      "Train Epochs: 2, Loss: 2.078768 \n",
      "Train Epochs: 2, Loss: 2.089092 \n",
      "Train Epochs: 2, Loss: 2.250988 \n",
      "Train Epochs: 2, Loss: 1.904721 \n",
      "Train Epochs: 2, Loss: 2.298623 \n",
      "Train Epochs: 2, Loss: 2.170226 \n",
      "Train Epochs: 2, Loss: 2.526897 \n",
      "Train Epochs: 2, Loss: 2.350477 \n",
      "Train Epochs: 2, Loss: 1.673693 \n",
      "Train Epochs: 2, Loss: 1.886433 \n",
      "Train Epochs: 2, Loss: 2.050844 \n",
      "Train Epochs: 2, Loss: 2.617366 \n",
      "Train Epochs: 2, Loss: 2.078782 \n",
      "Train Epochs: 2, Loss: 2.589821 \n",
      "Train Epochs: 2, Loss: 2.341657 \n",
      "Train Epochs: 2, Loss: 2.452080 \n",
      "Train Epochs: 2, Loss: 2.105369 \n",
      "Train Epochs: 2, Loss: 2.306980 \n",
      "Train Epochs: 2, Loss: 1.962010 \n",
      "Train Epochs: 2, Loss: 2.380696 \n",
      "Train Epochs: 2, Loss: 2.565837 \n",
      "Train Epochs: 2, Loss: 1.708173 \n",
      "Train Epochs: 2, Loss: 2.122443 \n",
      "Train Epochs: 2, Loss: 2.502200 \n",
      "Train Epochs: 2, Loss: 2.152435 \n",
      "Train Epochs: 2, Loss: 2.067893 \n",
      "Train Epochs: 2, Loss: 2.553691 \n",
      "Train Epochs: 2, Loss: 2.158269 \n",
      "Train Epochs: 2, Loss: 1.983136 \n",
      "Train Epochs: 2, Loss: 2.478550 \n",
      "Train Epochs: 2, Loss: 2.036702 \n",
      "Train Epochs: 2, Loss: 2.153479 \n",
      "Train Epochs: 2, Loss: 2.516537 \n",
      "Train Epochs: 2, Loss: 1.924022 \n",
      "Train Epochs: 2, Loss: 2.602526 \n",
      "Train Epochs: 2, Loss: 2.294744 \n",
      "Train Epochs: 2, Loss: 2.237368 \n",
      "Train Epochs: 2, Loss: 2.358101 \n",
      "Train Epochs: 2, Loss: 2.226077 \n",
      "Train Epochs: 2, Loss: 2.153869 \n",
      "Train Epochs: 2, Loss: 2.550323 \n",
      "Train Epochs: 2, Loss: 1.840400 \n",
      "Train Epochs: 2, Loss: 2.021311 \n",
      "Train Epochs: 2, Loss: 2.246063 \n",
      "Train Epochs: 2, Loss: 2.670830 \n",
      "Train Epochs: 2, Loss: 2.772876 \n",
      "Train Epochs: 2, Loss: 2.340513 \n",
      "Train Epochs: 2, Loss: 2.460297 \n",
      "Train Epochs: 2, Loss: 2.311736 \n",
      "Train Epochs: 2, Loss: 2.602164 \n",
      "Train Epochs: 2, Loss: 2.219970 \n",
      "Train Epochs: 2, Loss: 2.149913 \n",
      "Train Epochs: 2, Loss: 2.153447 \n",
      "Train Epochs: 2, Loss: 2.779268 \n",
      "Train Epochs: 2, Loss: 2.000448 \n",
      "Train Epochs: 2, Loss: 2.428170 \n",
      "\n",
      "Average Loss: 2.2674, Accuracy: 58\n",
      "Train Epochs: 3, Loss: 2.276110 \n",
      "Train Epochs: 3, Loss: 2.289386 \n",
      "Train Epochs: 3, Loss: 2.600512 \n",
      "Train Epochs: 3, Loss: 2.592838 \n",
      "Train Epochs: 3, Loss: 2.145839 \n",
      "Train Epochs: 3, Loss: 2.288428 \n",
      "Train Epochs: 3, Loss: 2.070910 \n",
      "Train Epochs: 3, Loss: 2.176632 \n",
      "Train Epochs: 3, Loss: 2.386640 \n",
      "Train Epochs: 3, Loss: 2.140446 \n",
      "Train Epochs: 3, Loss: 1.704972 \n",
      "Train Epochs: 3, Loss: 1.908466 \n",
      "Train Epochs: 3, Loss: 2.296605 \n",
      "Train Epochs: 3, Loss: 2.343638 \n",
      "Train Epochs: 3, Loss: 2.575046 \n",
      "Train Epochs: 3, Loss: 2.462703 \n",
      "Train Epochs: 3, Loss: 1.932509 \n",
      "Train Epochs: 3, Loss: 2.196455 \n",
      "Train Epochs: 3, Loss: 2.018101 \n",
      "Train Epochs: 3, Loss: 2.904393 \n",
      "Train Epochs: 3, Loss: 2.243558 \n",
      "Train Epochs: 3, Loss: 1.948826 \n",
      "Train Epochs: 3, Loss: 2.584165 \n",
      "Train Epochs: 3, Loss: 2.490369 \n",
      "Train Epochs: 3, Loss: 2.129243 \n",
      "Train Epochs: 3, Loss: 2.101760 \n",
      "Train Epochs: 3, Loss: 2.092018 \n",
      "Train Epochs: 3, Loss: 2.169412 \n",
      "Train Epochs: 3, Loss: 2.122183 \n",
      "Train Epochs: 3, Loss: 2.338033 \n",
      "Train Epochs: 3, Loss: 2.630769 \n",
      "Train Epochs: 3, Loss: 2.190627 \n",
      "Train Epochs: 3, Loss: 2.007711 \n",
      "Train Epochs: 3, Loss: 2.284265 \n",
      "Train Epochs: 3, Loss: 2.219978 \n",
      "Train Epochs: 3, Loss: 2.109627 \n",
      "Train Epochs: 3, Loss: 2.701304 \n",
      "Train Epochs: 3, Loss: 2.416820 \n",
      "Train Epochs: 3, Loss: 2.426546 \n",
      "Train Epochs: 3, Loss: 2.076157 \n",
      "Train Epochs: 3, Loss: 2.351879 \n",
      "Train Epochs: 3, Loss: 2.183928 \n",
      "Train Epochs: 3, Loss: 2.165543 \n",
      "Train Epochs: 3, Loss: 2.821059 \n",
      "Train Epochs: 3, Loss: 2.444462 \n",
      "Train Epochs: 3, Loss: 2.087071 \n",
      "Train Epochs: 3, Loss: 2.400585 \n",
      "Train Epochs: 3, Loss: 1.865239 \n",
      "Train Epochs: 3, Loss: 2.167346 \n",
      "Train Epochs: 3, Loss: 2.227401 \n",
      "Train Epochs: 3, Loss: 1.864642 \n",
      "Train Epochs: 3, Loss: 2.349238 \n",
      "Train Epochs: 3, Loss: 1.739935 \n",
      "Train Epochs: 3, Loss: 1.966049 \n",
      "Train Epochs: 3, Loss: 2.066665 \n",
      "Train Epochs: 3, Loss: 2.169320 \n",
      "Train Epochs: 3, Loss: 2.313556 \n",
      "Train Epochs: 3, Loss: 2.364807 \n",
      "Train Epochs: 3, Loss: 2.179292 \n",
      "Train Epochs: 3, Loss: 2.311109 \n",
      "\n",
      "Average Loss: 2.2700, Accuracy: 58\n",
      "Train Epochs: 4, Loss: 2.509986 \n",
      "Train Epochs: 4, Loss: 2.301404 \n",
      "Train Epochs: 4, Loss: 1.918553 \n",
      "Train Epochs: 4, Loss: 1.959266 \n",
      "Train Epochs: 4, Loss: 2.361040 \n",
      "Train Epochs: 4, Loss: 1.944698 \n",
      "Train Epochs: 4, Loss: 2.380056 \n",
      "Train Epochs: 4, Loss: 2.319007 \n",
      "Train Epochs: 4, Loss: 2.011324 \n",
      "Train Epochs: 4, Loss: 2.297616 \n",
      "Train Epochs: 4, Loss: 1.779170 \n",
      "Train Epochs: 4, Loss: 2.232432 \n",
      "Train Epochs: 4, Loss: 2.393813 \n",
      "Train Epochs: 4, Loss: 2.244401 \n",
      "Train Epochs: 4, Loss: 2.213088 \n",
      "Train Epochs: 4, Loss: 2.021049 \n",
      "Train Epochs: 4, Loss: 2.450144 \n",
      "Train Epochs: 4, Loss: 2.655037 \n",
      "Train Epochs: 4, Loss: 2.205182 \n",
      "Train Epochs: 4, Loss: 2.309577 \n",
      "Train Epochs: 4, Loss: 2.279138 \n",
      "Train Epochs: 4, Loss: 2.468085 \n",
      "Train Epochs: 4, Loss: 2.053041 \n",
      "Train Epochs: 4, Loss: 2.552899 \n",
      "Train Epochs: 4, Loss: 1.865578 \n",
      "Train Epochs: 4, Loss: 1.789889 \n",
      "Train Epochs: 4, Loss: 1.825378 \n",
      "Train Epochs: 4, Loss: 2.610674 \n",
      "Train Epochs: 4, Loss: 2.099120 \n",
      "Train Epochs: 4, Loss: 2.072536 \n",
      "Train Epochs: 4, Loss: 2.665863 \n",
      "Train Epochs: 4, Loss: 2.359285 \n",
      "Train Epochs: 4, Loss: 2.310184 \n",
      "Train Epochs: 4, Loss: 2.481600 \n",
      "Train Epochs: 4, Loss: 2.320541 \n",
      "Train Epochs: 4, Loss: 2.248081 \n",
      "Train Epochs: 4, Loss: 2.654959 \n",
      "Train Epochs: 4, Loss: 2.040707 \n",
      "Train Epochs: 4, Loss: 1.917199 \n",
      "Train Epochs: 4, Loss: 2.421392 \n",
      "Train Epochs: 4, Loss: 2.157934 \n",
      "Train Epochs: 4, Loss: 2.123504 \n",
      "Train Epochs: 4, Loss: 1.959828 \n",
      "Train Epochs: 4, Loss: 2.084829 \n",
      "Train Epochs: 4, Loss: 2.541324 \n",
      "Train Epochs: 4, Loss: 1.784270 \n",
      "Train Epochs: 4, Loss: 2.063204 \n",
      "Train Epochs: 4, Loss: 2.464663 \n",
      "Train Epochs: 4, Loss: 2.020326 \n",
      "Train Epochs: 4, Loss: 1.866876 \n",
      "Train Epochs: 4, Loss: 1.865516 \n",
      "Train Epochs: 4, Loss: 2.243737 \n",
      "Train Epochs: 4, Loss: 2.409761 \n",
      "Train Epochs: 4, Loss: 2.353091 \n",
      "Train Epochs: 4, Loss: 2.427201 \n",
      "Train Epochs: 4, Loss: 2.032854 \n",
      "Train Epochs: 4, Loss: 2.107494 \n",
      "Train Epochs: 4, Loss: 2.185603 \n",
      "Train Epochs: 4, Loss: 2.166066 \n",
      "Train Epochs: 4, Loss: 2.314917 \n",
      "\n",
      "Average Loss: 2.2443, Accuracy: 58\n",
      "Train Epochs: 5, Loss: 2.227505 \n",
      "Train Epochs: 5, Loss: 2.260269 \n",
      "Train Epochs: 5, Loss: 2.358459 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epochs: 5, Loss: 2.482595 \n",
      "Train Epochs: 5, Loss: 2.464866 \n",
      "Train Epochs: 5, Loss: 2.469831 \n",
      "Train Epochs: 5, Loss: 2.504758 \n",
      "Train Epochs: 5, Loss: 2.519483 \n",
      "Train Epochs: 5, Loss: 1.917052 \n",
      "Train Epochs: 5, Loss: 1.986126 \n",
      "Train Epochs: 5, Loss: 2.157100 \n",
      "Train Epochs: 5, Loss: 1.600384 \n",
      "Train Epochs: 5, Loss: 2.057398 \n",
      "Train Epochs: 5, Loss: 2.408662 \n",
      "Train Epochs: 5, Loss: 1.663477 \n",
      "Train Epochs: 5, Loss: 2.508080 \n",
      "Train Epochs: 5, Loss: 2.324781 \n",
      "Train Epochs: 5, Loss: 2.346387 \n",
      "Train Epochs: 5, Loss: 2.384036 \n",
      "Train Epochs: 5, Loss: 2.066051 \n",
      "Train Epochs: 5, Loss: 2.443524 \n",
      "Train Epochs: 5, Loss: 1.828758 \n",
      "Train Epochs: 5, Loss: 1.946257 \n",
      "Train Epochs: 5, Loss: 1.972721 \n",
      "Train Epochs: 5, Loss: 2.572272 \n",
      "Train Epochs: 5, Loss: 1.795892 \n",
      "Train Epochs: 5, Loss: 2.516432 \n",
      "Train Epochs: 5, Loss: 2.071373 \n",
      "Train Epochs: 5, Loss: 1.816190 \n",
      "Train Epochs: 5, Loss: 2.337108 \n",
      "Train Epochs: 5, Loss: 2.230922 \n",
      "Train Epochs: 5, Loss: 1.911889 \n",
      "Train Epochs: 5, Loss: 2.249379 \n",
      "Train Epochs: 5, Loss: 2.760723 \n",
      "Train Epochs: 5, Loss: 2.193814 \n",
      "Train Epochs: 5, Loss: 1.919986 \n",
      "Train Epochs: 5, Loss: 2.463270 \n",
      "Train Epochs: 5, Loss: 1.968965 \n",
      "Train Epochs: 5, Loss: 2.080814 \n",
      "Train Epochs: 5, Loss: 1.702938 \n",
      "Train Epochs: 5, Loss: 2.395921 \n",
      "Train Epochs: 5, Loss: 2.059814 \n",
      "Train Epochs: 5, Loss: 1.918551 \n",
      "Train Epochs: 5, Loss: 2.650616 \n",
      "Train Epochs: 5, Loss: 2.281931 \n",
      "Train Epochs: 5, Loss: 2.335897 \n",
      "Train Epochs: 5, Loss: 2.397190 \n",
      "Train Epochs: 5, Loss: 2.057218 \n",
      "Train Epochs: 5, Loss: 2.078411 \n",
      "Train Epochs: 5, Loss: 2.565545 \n",
      "Train Epochs: 5, Loss: 2.240663 \n",
      "Train Epochs: 5, Loss: 2.341181 \n",
      "Train Epochs: 5, Loss: 2.022844 \n",
      "Train Epochs: 5, Loss: 2.137861 \n",
      "Train Epochs: 5, Loss: 2.229385 \n",
      "Train Epochs: 5, Loss: 2.422011 \n",
      "Train Epochs: 5, Loss: 1.880926 \n",
      "Train Epochs: 5, Loss: 2.089768 \n",
      "Train Epochs: 5, Loss: 2.239347 \n",
      "Train Epochs: 5, Loss: 1.756584 \n",
      "\n",
      "Average Loss: 2.2483, Accuracy: 58\n",
      "Train Epochs: 6, Loss: 2.241983 \n",
      "Train Epochs: 6, Loss: 2.407331 \n",
      "Train Epochs: 6, Loss: 2.405635 \n",
      "Train Epochs: 6, Loss: 2.620114 \n",
      "Train Epochs: 6, Loss: 2.748352 \n",
      "Train Epochs: 6, Loss: 1.803072 \n",
      "Train Epochs: 6, Loss: 1.973800 \n",
      "Train Epochs: 6, Loss: 2.601429 \n",
      "Train Epochs: 6, Loss: 2.231083 \n",
      "Train Epochs: 6, Loss: 2.325754 \n",
      "Train Epochs: 6, Loss: 1.913771 \n",
      "Train Epochs: 6, Loss: 2.076825 \n",
      "Train Epochs: 6, Loss: 2.280264 \n",
      "Train Epochs: 6, Loss: 2.336339 \n",
      "Train Epochs: 6, Loss: 2.357387 \n",
      "Train Epochs: 6, Loss: 2.240202 \n",
      "Train Epochs: 6, Loss: 2.447791 \n",
      "Train Epochs: 6, Loss: 2.624691 \n",
      "Train Epochs: 6, Loss: 2.125830 \n",
      "Train Epochs: 6, Loss: 1.810571 \n",
      "Train Epochs: 6, Loss: 2.096619 \n",
      "Train Epochs: 6, Loss: 1.792855 \n",
      "Train Epochs: 6, Loss: 2.227368 \n",
      "Train Epochs: 6, Loss: 2.107673 \n",
      "Train Epochs: 6, Loss: 2.348608 \n",
      "Train Epochs: 6, Loss: 2.386629 \n",
      "Train Epochs: 6, Loss: 2.505843 \n",
      "Train Epochs: 6, Loss: 1.350254 \n",
      "Train Epochs: 6, Loss: 1.978261 \n",
      "Train Epochs: 6, Loss: 2.157838 \n",
      "Train Epochs: 6, Loss: 2.020025 \n",
      "Train Epochs: 6, Loss: 2.495106 \n",
      "Train Epochs: 6, Loss: 2.263596 \n",
      "Train Epochs: 6, Loss: 2.308704 \n",
      "Train Epochs: 6, Loss: 2.207082 \n",
      "Train Epochs: 6, Loss: 2.468402 \n",
      "Train Epochs: 6, Loss: 2.642270 \n",
      "Train Epochs: 6, Loss: 1.789896 \n",
      "Train Epochs: 6, Loss: 2.086808 \n",
      "Train Epochs: 6, Loss: 2.226574 \n",
      "Train Epochs: 6, Loss: 2.015257 \n",
      "Train Epochs: 6, Loss: 1.965497 \n",
      "Train Epochs: 6, Loss: 2.137532 \n",
      "Train Epochs: 6, Loss: 2.240893 \n",
      "Train Epochs: 6, Loss: 2.088403 \n",
      "Train Epochs: 6, Loss: 2.389281 \n",
      "Train Epochs: 6, Loss: 2.740745 \n",
      "Train Epochs: 6, Loss: 2.125576 \n",
      "Train Epochs: 6, Loss: 2.094634 \n",
      "Train Epochs: 6, Loss: 2.022182 \n",
      "Train Epochs: 6, Loss: 2.554051 \n",
      "Train Epochs: 6, Loss: 2.407965 \n",
      "Train Epochs: 6, Loss: 2.183725 \n",
      "Train Epochs: 6, Loss: 1.967650 \n",
      "Train Epochs: 6, Loss: 2.344406 \n",
      "Train Epochs: 6, Loss: 2.550603 \n",
      "Train Epochs: 6, Loss: 2.298370 \n",
      "Train Epochs: 6, Loss: 2.140750 \n",
      "Train Epochs: 6, Loss: 1.883608 \n",
      "Train Epochs: 6, Loss: 2.642253 \n",
      "\n",
      "Average Loss: 2.2649, Accuracy: 58\n",
      "Train Epochs: 7, Loss: 2.421160 \n",
      "Train Epochs: 7, Loss: 1.975785 \n",
      "Train Epochs: 7, Loss: 2.444066 \n",
      "Train Epochs: 7, Loss: 2.332577 \n",
      "Train Epochs: 7, Loss: 1.910667 \n",
      "Train Epochs: 7, Loss: 2.230292 \n",
      "Train Epochs: 7, Loss: 1.879866 \n",
      "Train Epochs: 7, Loss: 2.122479 \n",
      "Train Epochs: 7, Loss: 1.815677 \n",
      "Train Epochs: 7, Loss: 2.582019 \n",
      "Train Epochs: 7, Loss: 2.281856 \n",
      "Train Epochs: 7, Loss: 2.550830 \n",
      "Train Epochs: 7, Loss: 2.425793 \n",
      "Train Epochs: 7, Loss: 2.199968 \n",
      "Train Epochs: 7, Loss: 1.955531 \n",
      "Train Epochs: 7, Loss: 1.867489 \n",
      "Train Epochs: 7, Loss: 1.990523 \n",
      "Train Epochs: 7, Loss: 2.332516 \n",
      "Train Epochs: 7, Loss: 1.996261 \n",
      "Train Epochs: 7, Loss: 2.288231 \n",
      "Train Epochs: 7, Loss: 2.231374 \n",
      "Train Epochs: 7, Loss: 1.562822 \n",
      "Train Epochs: 7, Loss: 2.032374 \n",
      "Train Epochs: 7, Loss: 2.184497 \n",
      "Train Epochs: 7, Loss: 2.288012 \n",
      "Train Epochs: 7, Loss: 2.322568 \n",
      "Train Epochs: 7, Loss: 2.098976 \n",
      "Train Epochs: 7, Loss: 2.070791 \n",
      "Train Epochs: 7, Loss: 2.491567 \n",
      "Train Epochs: 7, Loss: 2.663708 \n",
      "Train Epochs: 7, Loss: 2.250057 \n",
      "Train Epochs: 7, Loss: 2.714089 \n",
      "Train Epochs: 7, Loss: 1.993276 \n",
      "Train Epochs: 7, Loss: 2.408528 \n",
      "Train Epochs: 7, Loss: 2.213519 \n",
      "Train Epochs: 7, Loss: 2.251915 \n",
      "Train Epochs: 7, Loss: 2.375720 \n",
      "Train Epochs: 7, Loss: 2.015322 \n",
      "Train Epochs: 7, Loss: 1.881307 \n",
      "Train Epochs: 7, Loss: 2.177480 \n",
      "Train Epochs: 7, Loss: 2.617689 \n",
      "Train Epochs: 7, Loss: 2.089493 \n",
      "Train Epochs: 7, Loss: 2.092186 \n",
      "Train Epochs: 7, Loss: 2.374740 \n",
      "Train Epochs: 7, Loss: 2.724369 \n",
      "Train Epochs: 7, Loss: 1.879670 \n",
      "Train Epochs: 7, Loss: 2.191453 \n",
      "Train Epochs: 7, Loss: 1.882419 \n",
      "Train Epochs: 7, Loss: 2.026849 \n",
      "Train Epochs: 7, Loss: 2.279936 \n",
      "Train Epochs: 7, Loss: 2.284114 \n",
      "Train Epochs: 7, Loss: 2.374476 \n",
      "Train Epochs: 7, Loss: 1.723396 \n",
      "Train Epochs: 7, Loss: 2.302821 \n",
      "Train Epochs: 7, Loss: 2.280076 \n",
      "Train Epochs: 7, Loss: 2.346268 \n",
      "Train Epochs: 7, Loss: 2.135855 \n",
      "Train Epochs: 7, Loss: 2.668266 \n",
      "Train Epochs: 7, Loss: 2.399953 \n",
      "Train Epochs: 7, Loss: 2.789658 \n",
      "\n",
      "Average Loss: 2.2581, Accuracy: 58\n",
      "Train Epochs: 8, Loss: 2.237373 \n",
      "Train Epochs: 8, Loss: 2.230406 \n",
      "Train Epochs: 8, Loss: 2.292480 \n",
      "Train Epochs: 8, Loss: 2.513266 \n",
      "Train Epochs: 8, Loss: 2.106075 \n",
      "Train Epochs: 8, Loss: 1.870346 \n",
      "Train Epochs: 8, Loss: 2.662629 \n",
      "Train Epochs: 8, Loss: 2.340169 \n",
      "Train Epochs: 8, Loss: 2.728935 \n",
      "Train Epochs: 8, Loss: 2.138207 \n",
      "Train Epochs: 8, Loss: 1.893743 \n",
      "Train Epochs: 8, Loss: 1.409016 \n",
      "Train Epochs: 8, Loss: 2.704097 \n",
      "Train Epochs: 8, Loss: 2.392132 \n",
      "Train Epochs: 8, Loss: 2.150234 \n",
      "Train Epochs: 8, Loss: 2.341388 \n",
      "Train Epochs: 8, Loss: 2.227164 \n",
      "Train Epochs: 8, Loss: 1.947695 \n",
      "Train Epochs: 8, Loss: 2.143047 \n",
      "Train Epochs: 8, Loss: 2.575029 \n",
      "Train Epochs: 8, Loss: 1.975104 \n",
      "Train Epochs: 8, Loss: 2.055110 \n",
      "Train Epochs: 8, Loss: 2.038920 \n",
      "Train Epochs: 8, Loss: 2.123382 \n",
      "Train Epochs: 8, Loss: 2.342833 \n",
      "Train Epochs: 8, Loss: 1.765703 \n",
      "Train Epochs: 8, Loss: 1.876699 \n",
      "Train Epochs: 8, Loss: 2.102655 \n",
      "Train Epochs: 8, Loss: 1.803701 \n",
      "Train Epochs: 8, Loss: 1.867508 \n",
      "Train Epochs: 8, Loss: 2.756540 \n",
      "Train Epochs: 8, Loss: 1.606761 \n",
      "Train Epochs: 8, Loss: 1.842162 \n",
      "Train Epochs: 8, Loss: 1.939535 \n",
      "Train Epochs: 8, Loss: 1.771636 \n",
      "Train Epochs: 8, Loss: 1.777841 \n",
      "Train Epochs: 8, Loss: 2.633762 \n",
      "Train Epochs: 8, Loss: 1.939944 \n",
      "Train Epochs: 8, Loss: 1.873534 \n",
      "Train Epochs: 8, Loss: 1.698798 \n",
      "Train Epochs: 8, Loss: 1.341854 \n",
      "Train Epochs: 8, Loss: 1.709750 \n",
      "Train Epochs: 8, Loss: 1.715829 \n",
      "Train Epochs: 8, Loss: 2.028711 \n",
      "Train Epochs: 8, Loss: 2.193007 \n",
      "Train Epochs: 8, Loss: 1.699077 \n",
      "Train Epochs: 8, Loss: 1.813036 \n",
      "Train Epochs: 8, Loss: 1.891894 \n",
      "Train Epochs: 8, Loss: 1.825298 \n",
      "Train Epochs: 8, Loss: 1.585652 \n",
      "Train Epochs: 8, Loss: 1.443789 \n",
      "Train Epochs: 8, Loss: 1.462804 \n",
      "Train Epochs: 8, Loss: 1.552879 \n",
      "Train Epochs: 8, Loss: 1.576933 \n",
      "Train Epochs: 8, Loss: 1.866941 \n",
      "Train Epochs: 8, Loss: 1.635678 \n",
      "Train Epochs: 8, Loss: 1.513446 \n",
      "Train Epochs: 8, Loss: 1.466201 \n",
      "Train Epochs: 8, Loss: 1.616052 \n",
      "Train Epochs: 8, Loss: 1.387447 \n",
      "\n",
      "Average Loss: 1.7601, Accuracy: 68\n",
      "Train Epochs: 9, Loss: 1.976762 \n",
      "Train Epochs: 9, Loss: 1.509196 \n",
      "Train Epochs: 9, Loss: 1.461956 \n",
      "Train Epochs: 9, Loss: 1.507142 \n",
      "Train Epochs: 9, Loss: 1.475888 \n",
      "Train Epochs: 9, Loss: 1.854595 \n",
      "Train Epochs: 9, Loss: 1.677373 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epochs: 9, Loss: 1.610819 \n",
      "Train Epochs: 9, Loss: 1.739968 \n",
      "Train Epochs: 9, Loss: 1.603019 \n",
      "Train Epochs: 9, Loss: 1.604754 \n",
      "Train Epochs: 9, Loss: 1.932832 \n",
      "Train Epochs: 9, Loss: 1.746282 \n",
      "Train Epochs: 9, Loss: 1.166494 \n",
      "Train Epochs: 9, Loss: 1.584717 \n",
      "Train Epochs: 9, Loss: 1.174991 \n",
      "Train Epochs: 9, Loss: 1.906915 \n",
      "Train Epochs: 9, Loss: 1.445309 \n",
      "Train Epochs: 9, Loss: 2.116418 \n",
      "Train Epochs: 9, Loss: 1.807498 \n",
      "Train Epochs: 9, Loss: 1.767291 \n",
      "Train Epochs: 9, Loss: 1.678322 \n",
      "Train Epochs: 9, Loss: 2.046340 \n",
      "Train Epochs: 9, Loss: 1.500889 \n",
      "Train Epochs: 9, Loss: 1.534151 \n",
      "Train Epochs: 9, Loss: 1.368929 \n",
      "Train Epochs: 9, Loss: 1.603586 \n",
      "Train Epochs: 9, Loss: 1.539280 \n",
      "Train Epochs: 9, Loss: 1.746087 \n",
      "Train Epochs: 9, Loss: 1.597025 \n",
      "Train Epochs: 9, Loss: 1.539165 \n",
      "Train Epochs: 9, Loss: 1.398569 \n",
      "Train Epochs: 9, Loss: 2.040810 \n",
      "Train Epochs: 9, Loss: 1.403992 \n",
      "Train Epochs: 9, Loss: 1.786887 \n",
      "Train Epochs: 9, Loss: 1.552739 \n",
      "Train Epochs: 9, Loss: 1.862036 \n",
      "Train Epochs: 9, Loss: 1.592847 \n",
      "Train Epochs: 9, Loss: 1.598378 \n",
      "Train Epochs: 9, Loss: 1.758402 \n",
      "Train Epochs: 9, Loss: 2.012347 \n",
      "Train Epochs: 9, Loss: 1.206030 \n",
      "Train Epochs: 9, Loss: 1.823655 \n",
      "Train Epochs: 9, Loss: 1.651571 \n",
      "Train Epochs: 9, Loss: 1.601377 \n",
      "Train Epochs: 9, Loss: 2.163172 \n",
      "Train Epochs: 9, Loss: 1.445329 \n",
      "Train Epochs: 9, Loss: 2.206580 \n",
      "Train Epochs: 9, Loss: 1.849892 \n",
      "Train Epochs: 9, Loss: 2.130640 \n",
      "Train Epochs: 9, Loss: 1.670074 \n",
      "Train Epochs: 9, Loss: 1.609619 \n",
      "Train Epochs: 9, Loss: 1.892991 \n",
      "Train Epochs: 9, Loss: 1.594260 \n",
      "Train Epochs: 9, Loss: 1.872993 \n",
      "Train Epochs: 9, Loss: 1.796952 \n",
      "Train Epochs: 9, Loss: 1.563408 \n",
      "Train Epochs: 9, Loss: 1.768536 \n",
      "Train Epochs: 9, Loss: 1.464968 \n",
      "Train Epochs: 9, Loss: 1.521677 \n",
      "\n",
      "Average Loss: 1.7616, Accuracy: 68\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs): # train epoch = 10\n",
    "        model.train() #training\n",
    "        for batch_idx, (data, label) in enumerate(train_loader): #enumerate train_loader per batch-> index, (data, label) ex: 0, (img1, 4)... 1, (img2, 2)\n",
    "            data, label = Variable(data), Variable(label) #create torch variable and enter each data and label into it\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data) #enter data into model, save in output\n",
    "            train_loss = F.nll_loss(output, label) #nll = negative log likehood loss between output and label. it useful for classification problem with n class\n",
    "            train_loss.backward() #compute gradient\n",
    "            optimizer.step() #update weight\n",
    "            if batch_idx % 10 == 0: #display step\n",
    "                print('Train Epochs: {}, Loss: {:.6f} '.format(epoch, train_loss.data )) #print\n",
    "        model.eval() #evaluation\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        for data, label in validation_loader: #separate data and label\n",
    "            data, label = Variable(data,volatile=True), Variable(label) #create torch variable and enter data and label into it\n",
    "            output = model(data) #enter data into model, save in output\n",
    "            test_loss += F.nll_loss(output, label, size_average=False).data #\n",
    "            pred = output.data.max(1, keepdim=True)[1] #prediction result\n",
    "            correct += pred.eq(label.data.view_as(pred)).cpu().sum() #if label=pred then correct++\n",
    "        test_loss /= len(validation_loader.dataset) #compute test loss\n",
    "        print('\\nAverage Loss: {:.4f}, Accuracy: {:.0f}'.format(test_loss,  100. * correct / len(validation_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2iCKQhka5cGX"
   },
   "source": [
    "#### predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "11Qge0PVWlG0",
    "outputId": "ee0fa33e-bd4c-4b9d-c46d-03201ed16199"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Digit = 7\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "images, labels = next(iter(validation_loader))# load an image from validation_loader\n",
    "img = images[0].view(1, 784) \n",
    "with torch.no_grad():\n",
    "    logps = model(img)\n",
    "ps = torch.exp(logps)\n",
    "probab = list(ps.numpy()[0])\n",
    "print(\"Predicted Digit =\", probab.index(max(probab)))\n",
    "img = np.array(img, dtype='float')\n",
    "pixels = img.reshape((28, 28))\n",
    "plt.imshow(pixels, cmap='gray')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
